{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, MinMaxScaler, LabelBinarizer, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_titanic_processing(data):\n",
    "    # retirando dados desnecessários e divindo dados e labels\n",
    "    data.drop(labels=['Name', 'Ticket', 'PassengerId'],axis=1, inplace=True)\n",
    "    data = data.dropna()\n",
    "    data_x, data_y = data.drop(labels='Survived', axis=1), data['Survived']\n",
    "    data_y = np.array(data_y).reshape(-1,1)\n",
    "    \n",
    "    # Tratando dados númericos e categoricos\n",
    "    data_x_numerical = data_x.drop(labels=['Sex', 'Cabin', 'Embarked'], axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    data_x_numerical[['Age', 'Fare']] = scaler.fit_transform(data_x_numerical[['Age', 'Fare']])\n",
    "    data_x_categorical = data_x[['Sex', 'Cabin', 'Embarked']]\n",
    "    binarizer = LabelBinarizer()\n",
    "    encoder = LabelEncoder()\n",
    "    data_x_categorical['Sex'] = binarizer.fit_transform(data_x_categorical['Sex'])\n",
    "    data_x_categorical['Cabin'] = encoder.fit_transform(data_x_categorical['Cabin'])\n",
    "    data_x_categorical['Cabin'] = scaler.fit_transform(data_x_categorical['Cabin'].reshape(-1,1))\n",
    "    data_x_categorical['Embarked'] = encoder.fit_transform(data_x_categorical['Embarked'])\n",
    "    \n",
    "    # Merge dos dados categoricos e númericos tratados\n",
    "    data_x_categorical = data_x_categorical.reset_index()\n",
    "    data_x_numerical = data_x_numerical.reset_index()\n",
    "    data_tr = data_x_categorical.merge(data_x_numerical, on='index')\n",
    "    data_tr.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    return data_tr, data_y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_test_titanic_processing(data):\n",
    "    # retirando dados desnecessários e divindo dados e labels\n",
    "    data.drop(labels=['name', 'ticket', 'boat', 'body', 'home.dest'],axis=1, inplace=True)\n",
    "    data = data.dropna()\n",
    "    data_x, data_y = data.drop(labels='survived', axis=1), data['survived']\n",
    "    data_y = np.array(data_y).reshape(-1,1)\n",
    "    \n",
    "    # Tratando dados númericos e categoricos\n",
    "    data_x_numerical = data_x.drop(labels=['sex', 'cabin', 'embarked'], axis=1)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    data_x_numerical[['age', 'fare']] = scaler.fit_transform(data_x_numerical[['age', 'fare']])\n",
    "    data_x_categorical = data_x[['sex', 'cabin', 'embarked']]\n",
    "    binarizer = LabelBinarizer()\n",
    "    encoder = LabelEncoder()\n",
    "    data_x_categorical['sex'] = binarizer.fit_transform(data_x_categorical['sex'])\n",
    "    data_x_categorical['cabin'] = encoder.fit_transform(data_x_categorical['cabin'])\n",
    "    data_x_categorical['cabin'] = scaler.fit_transform(data_x_categorical['cabin'].reshape(-1,1))\n",
    "    data_x_categorical['embarked'] = encoder.fit_transform(data_x_categorical['embarked'])\n",
    "    \n",
    "    # Merge dos dados categoricos e númericos tratados\n",
    "    data_x_categorical = data_x_categorical.reset_index()\n",
    "    data_x_numerical = data_x_numerical.reset_index()\n",
    "    data_tr = data_x_categorical.merge(data_x_numerical, on='index')\n",
    "    data_tr.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    return data_tr, data_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_excel('dataset/titanic3.xls')\n",
    "data_test_x, data_test_y = data_test_titanic_processing(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Programas\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/train.csv')\n",
    "data_x, data_y = data_titanic_processing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train_x, data_test_x, data_train_y, data_test_y = train_test_split(data_x, data_y, test_size=83, random_state=42)\n",
    "data_train_x.shape, data_test_x.shape, data_train_y.shape, data_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y_oh, data_test_y_oh = to_categorical(data_y, 2), to_categorical(data_test_y, 2)\n",
    "data_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_179 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,934\n",
      "Trainable params: 1,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 183 samples, validate on 270 samples\n",
      "Epoch 1/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.6544 - acc: 0.6721 - val_loss: 0.6325 - val_acc: 0.6667\n",
      "Epoch 2/1000\n",
      "183/183 [==============================] - 0s 310us/step - loss: 0.6270 - acc: 0.6721 - val_loss: 0.6127 - val_acc: 0.6667\n",
      "Epoch 3/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.6131 - acc: 0.6721 - val_loss: 0.5941 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.5993 - acc: 0.6776 - val_loss: 0.5773 - val_acc: 0.7222\n",
      "Epoch 5/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.5808 - acc: 0.7268 - val_loss: 0.5615 - val_acc: 0.7074\n",
      "Epoch 6/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.5668 - acc: 0.6995 - val_loss: 0.5495 - val_acc: 0.6852\n",
      "Epoch 7/1000\n",
      "183/183 [==============================] - 0s 320us/step - loss: 0.5556 - acc: 0.6995 - val_loss: 0.5339 - val_acc: 0.7333\n",
      "Epoch 8/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.5421 - acc: 0.7268 - val_loss: 0.5227 - val_acc: 0.7259\n",
      "Epoch 9/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.5306 - acc: 0.7377 - val_loss: 0.5112 - val_acc: 0.7333\n",
      "Epoch 10/1000\n",
      "183/183 [==============================] - 0s 345us/step - loss: 0.5200 - acc: 0.7213 - val_loss: 0.5018 - val_acc: 0.7333\n",
      "Epoch 11/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.5115 - acc: 0.7377 - val_loss: 0.4933 - val_acc: 0.7333\n",
      "Epoch 12/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.5030 - acc: 0.7268 - val_loss: 0.4867 - val_acc: 0.7333\n",
      "Epoch 13/1000\n",
      "183/183 [==============================] - 0s 337us/step - loss: 0.4982 - acc: 0.7377 - val_loss: 0.4820 - val_acc: 0.7333\n",
      "Epoch 14/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.4947 - acc: 0.7705 - val_loss: 0.4753 - val_acc: 0.7593\n",
      "Epoch 15/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.4882 - acc: 0.7541 - val_loss: 0.4711 - val_acc: 0.7519\n",
      "Epoch 16/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.4896 - acc: 0.7158 - val_loss: 0.4729 - val_acc: 0.7407\n",
      "Epoch 17/1000\n",
      "183/183 [==============================] - 0s 310us/step - loss: 0.4802 - acc: 0.7432 - val_loss: 0.4643 - val_acc: 0.7704\n",
      "Epoch 18/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.4822 - acc: 0.7705 - val_loss: 0.4616 - val_acc: 0.7704\n",
      "Epoch 19/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.4781 - acc: 0.7650 - val_loss: 0.4600 - val_acc: 0.7741\n",
      "Epoch 20/1000\n",
      "183/183 [==============================] - 0s 328us/step - loss: 0.4758 - acc: 0.7322 - val_loss: 0.4667 - val_acc: 0.7370\n",
      "Epoch 21/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.4774 - acc: 0.7322 - val_loss: 0.4581 - val_acc: 0.7741\n",
      "Epoch 22/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.4716 - acc: 0.7596 - val_loss: 0.4590 - val_acc: 0.7481\n",
      "Epoch 23/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.4711 - acc: 0.7541 - val_loss: 0.4558 - val_acc: 0.7704\n",
      "Epoch 24/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4724 - acc: 0.7377 - val_loss: 0.4587 - val_acc: 0.7370\n",
      "Epoch 25/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4709 - acc: 0.7268 - val_loss: 0.4525 - val_acc: 0.7667\n",
      "Epoch 26/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.4668 - acc: 0.7705 - val_loss: 0.4534 - val_acc: 0.7630\n",
      "Epoch 27/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.4614 - acc: 0.7596 - val_loss: 0.4512 - val_acc: 0.7667\n",
      "Epoch 28/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.4633 - acc: 0.7541 - val_loss: 0.4507 - val_acc: 0.7704\n",
      "Epoch 29/1000\n",
      "183/183 [==============================] - 0s 295us/step - loss: 0.4619 - acc: 0.7377 - val_loss: 0.4535 - val_acc: 0.7519\n",
      "Epoch 30/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4695 - acc: 0.6995 - val_loss: 0.4572 - val_acc: 0.7333\n",
      "Epoch 31/1000\n",
      "183/183 [==============================] - 0s 326us/step - loss: 0.4685 - acc: 0.7486 - val_loss: 0.4532 - val_acc: 0.7778\n",
      "Epoch 32/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.4637 - acc: 0.7705 - val_loss: 0.4484 - val_acc: 0.7704\n",
      "Epoch 33/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4559 - acc: 0.7486 - val_loss: 0.4561 - val_acc: 0.7519\n",
      "Epoch 34/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.4574 - acc: 0.7377 - val_loss: 0.4497 - val_acc: 0.7556\n",
      "Epoch 35/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.4566 - acc: 0.7596 - val_loss: 0.4474 - val_acc: 0.7926\n",
      "Epoch 36/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.4568 - acc: 0.7705 - val_loss: 0.4480 - val_acc: 0.7519\n",
      "Epoch 37/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.4530 - acc: 0.7596 - val_loss: 0.4458 - val_acc: 0.7704\n",
      "Epoch 38/1000\n",
      "183/183 [==============================] - 0s 342us/step - loss: 0.4513 - acc: 0.7596 - val_loss: 0.4466 - val_acc: 0.7667\n",
      "Epoch 39/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.4571 - acc: 0.7486 - val_loss: 0.4494 - val_acc: 0.7444\n",
      "Epoch 40/1000\n",
      "183/183 [==============================] - 0s 310us/step - loss: 0.4508 - acc: 0.7596 - val_loss: 0.4449 - val_acc: 0.7889\n",
      "Epoch 41/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.4514 - acc: 0.7596 - val_loss: 0.4444 - val_acc: 0.7667\n",
      "Epoch 42/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4474 - acc: 0.7596 - val_loss: 0.4433 - val_acc: 0.7704\n",
      "Epoch 43/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.4481 - acc: 0.7705 - val_loss: 0.4428 - val_acc: 0.7926\n",
      "Epoch 44/1000\n",
      "183/183 [==============================] - 0s 342us/step - loss: 0.4503 - acc: 0.7541 - val_loss: 0.4473 - val_acc: 0.7519\n",
      "Epoch 45/1000\n",
      "183/183 [==============================] - 0s 348us/step - loss: 0.4489 - acc: 0.7650 - val_loss: 0.4414 - val_acc: 0.7926\n",
      "Epoch 46/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.4465 - acc: 0.7705 - val_loss: 0.4434 - val_acc: 0.7630\n",
      "Epoch 47/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.4457 - acc: 0.7596 - val_loss: 0.4425 - val_acc: 0.7630\n",
      "Epoch 48/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.4507 - acc: 0.7760 - val_loss: 0.4413 - val_acc: 0.7815\n",
      "Epoch 49/1000\n",
      "183/183 [==============================] - 0s 321us/step - loss: 0.4461 - acc: 0.7596 - val_loss: 0.4472 - val_acc: 0.7481\n",
      "Epoch 50/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.4471 - acc: 0.7322 - val_loss: 0.4444 - val_acc: 0.7667\n",
      "Epoch 51/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.4467 - acc: 0.7705 - val_loss: 0.4407 - val_acc: 0.7778\n",
      "Epoch 52/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.4448 - acc: 0.7760 - val_loss: 0.4401 - val_acc: 0.7852\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 252us/step - loss: 0.4419 - acc: 0.7541 - val_loss: 0.4455 - val_acc: 0.7407\n",
      "Epoch 54/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.4426 - acc: 0.7541 - val_loss: 0.4410 - val_acc: 0.7815\n",
      "Epoch 55/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.4400 - acc: 0.7760 - val_loss: 0.4402 - val_acc: 0.7778\n",
      "Epoch 56/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.4415 - acc: 0.7760 - val_loss: 0.4414 - val_acc: 0.7778\n",
      "Epoch 57/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.4390 - acc: 0.7705 - val_loss: 0.4428 - val_acc: 0.7704\n",
      "Epoch 58/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.4395 - acc: 0.7650 - val_loss: 0.4393 - val_acc: 0.7852\n",
      "Epoch 59/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.4393 - acc: 0.7705 - val_loss: 0.4384 - val_acc: 0.7926\n",
      "Epoch 60/1000\n",
      "183/183 [==============================] - 0s 341us/step - loss: 0.4372 - acc: 0.7814 - val_loss: 0.4404 - val_acc: 0.7667\n",
      "Epoch 61/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.4456 - acc: 0.7213 - val_loss: 0.4414 - val_acc: 0.7593\n",
      "Epoch 62/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.4358 - acc: 0.7814 - val_loss: 0.4367 - val_acc: 0.7815\n",
      "Epoch 63/1000\n",
      "183/183 [==============================] - 0s 348us/step - loss: 0.4435 - acc: 0.7705 - val_loss: 0.4361 - val_acc: 0.7815\n",
      "Epoch 64/1000\n",
      "183/183 [==============================] - 0s 303us/step - loss: 0.4368 - acc: 0.7923 - val_loss: 0.4454 - val_acc: 0.7296\n",
      "Epoch 65/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.4415 - acc: 0.7322 - val_loss: 0.4405 - val_acc: 0.7630\n",
      "Epoch 66/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.4384 - acc: 0.7705 - val_loss: 0.4373 - val_acc: 0.7926\n",
      "Epoch 67/1000\n",
      "183/183 [==============================] - 0s 306us/step - loss: 0.4355 - acc: 0.7814 - val_loss: 0.4377 - val_acc: 0.7926\n",
      "Epoch 68/1000\n",
      "183/183 [==============================] - 0s 276us/step - loss: 0.4417 - acc: 0.7814 - val_loss: 0.4437 - val_acc: 0.7630\n",
      "Epoch 69/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.4425 - acc: 0.7541 - val_loss: 0.4373 - val_acc: 0.7815\n",
      "Epoch 70/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.4365 - acc: 0.7760 - val_loss: 0.4371 - val_acc: 0.7926\n",
      "Epoch 71/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4338 - acc: 0.7869 - val_loss: 0.4392 - val_acc: 0.7704\n",
      "Epoch 72/1000\n",
      "183/183 [==============================] - 0s 284us/step - loss: 0.4354 - acc: 0.7705 - val_loss: 0.4378 - val_acc: 0.7778\n",
      "Epoch 73/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4381 - acc: 0.7486 - val_loss: 0.4381 - val_acc: 0.7778\n",
      "Epoch 74/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4362 - acc: 0.7814 - val_loss: 0.4364 - val_acc: 0.7852\n",
      "Epoch 75/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.4340 - acc: 0.7760 - val_loss: 0.4375 - val_acc: 0.7889\n",
      "Epoch 76/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.4377 - acc: 0.7814 - val_loss: 0.4430 - val_acc: 0.7667\n",
      "Epoch 77/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.4337 - acc: 0.7760 - val_loss: 0.4360 - val_acc: 0.7889\n",
      "Epoch 78/1000\n",
      "183/183 [==============================] - 0s 230us/step - loss: 0.4351 - acc: 0.7814 - val_loss: 0.4357 - val_acc: 0.7926\n",
      "Epoch 79/1000\n",
      "183/183 [==============================] - 0s 246us/step - loss: 0.4392 - acc: 0.7432 - val_loss: 0.4407 - val_acc: 0.7593\n",
      "Epoch 80/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.4367 - acc: 0.7596 - val_loss: 0.4354 - val_acc: 0.7889\n",
      "Epoch 81/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.4345 - acc: 0.7814 - val_loss: 0.4371 - val_acc: 0.7778\n",
      "Epoch 82/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4311 - acc: 0.7869 - val_loss: 0.4365 - val_acc: 0.7778\n",
      "Epoch 83/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4307 - acc: 0.7814 - val_loss: 0.4341 - val_acc: 0.7852\n",
      "Epoch 84/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4310 - acc: 0.7760 - val_loss: 0.4349 - val_acc: 0.7963\n",
      "Epoch 85/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.4305 - acc: 0.7869 - val_loss: 0.4363 - val_acc: 0.7815\n",
      "Epoch 86/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.4295 - acc: 0.7814 - val_loss: 0.4373 - val_acc: 0.7778\n",
      "Epoch 87/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.4341 - acc: 0.7814 - val_loss: 0.4354 - val_acc: 0.7889\n",
      "Epoch 88/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.4299 - acc: 0.7869 - val_loss: 0.4389 - val_acc: 0.7704\n",
      "Epoch 89/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.4318 - acc: 0.7923 - val_loss: 0.4331 - val_acc: 0.7889\n",
      "Epoch 90/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.4295 - acc: 0.7814 - val_loss: 0.4337 - val_acc: 0.7963\n",
      "Epoch 91/1000\n",
      "183/183 [==============================] - 0s 297us/step - loss: 0.4286 - acc: 0.7814 - val_loss: 0.4415 - val_acc: 0.7667\n",
      "Epoch 92/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.4316 - acc: 0.7650 - val_loss: 0.4337 - val_acc: 0.7926\n",
      "Epoch 93/1000\n",
      "183/183 [==============================] - 0s 300us/step - loss: 0.4303 - acc: 0.7869 - val_loss: 0.4331 - val_acc: 0.7815\n",
      "Epoch 94/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4297 - acc: 0.7869 - val_loss: 0.4411 - val_acc: 0.7667\n",
      "Epoch 95/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4287 - acc: 0.7814 - val_loss: 0.4332 - val_acc: 0.7926\n",
      "Epoch 96/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.4276 - acc: 0.7869 - val_loss: 0.4327 - val_acc: 0.7963\n",
      "Epoch 97/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.4275 - acc: 0.7760 - val_loss: 0.4374 - val_acc: 0.7741\n",
      "Epoch 98/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4272 - acc: 0.7705 - val_loss: 0.4340 - val_acc: 0.7852\n",
      "Epoch 99/1000\n",
      "183/183 [==============================] - 0s 322us/step - loss: 0.4268 - acc: 0.7923 - val_loss: 0.4321 - val_acc: 0.8000\n",
      "Epoch 100/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.4264 - acc: 0.7978 - val_loss: 0.4338 - val_acc: 0.7815\n",
      "Epoch 101/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.4257 - acc: 0.7814 - val_loss: 0.4348 - val_acc: 0.7778\n",
      "Epoch 102/1000\n",
      "183/183 [==============================] - 0s 351us/step - loss: 0.4261 - acc: 0.7760 - val_loss: 0.4338 - val_acc: 0.7778\n",
      "Epoch 103/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.4250 - acc: 0.7814 - val_loss: 0.4324 - val_acc: 0.7963\n",
      "Epoch 104/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4322 - acc: 0.7650 - val_loss: 0.4347 - val_acc: 0.7778\n",
      "Epoch 105/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.4269 - acc: 0.7814 - val_loss: 0.4378 - val_acc: 0.7741\n",
      "Epoch 106/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.4298 - acc: 0.7814 - val_loss: 0.4398 - val_acc: 0.7704\n",
      "Epoch 107/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.4259 - acc: 0.7760 - val_loss: 0.4313 - val_acc: 0.7852\n",
      "Epoch 108/1000\n",
      "183/183 [==============================] - 0s 367us/step - loss: 0.4263 - acc: 0.7814 - val_loss: 0.4326 - val_acc: 0.7815\n",
      "Epoch 109/1000\n",
      "183/183 [==============================] - 0s 345us/step - loss: 0.4273 - acc: 0.7814 - val_loss: 0.4319 - val_acc: 0.7889\n",
      "Epoch 110/1000\n",
      "183/183 [==============================] - 0s 337us/step - loss: 0.4298 - acc: 0.7541 - val_loss: 0.4333 - val_acc: 0.7815\n",
      "Epoch 111/1000\n",
      "183/183 [==============================] - 0s 397us/step - loss: 0.4231 - acc: 0.7814 - val_loss: 0.4312 - val_acc: 0.8000\n",
      "Epoch 112/1000\n",
      "183/183 [==============================] - 0s 326us/step - loss: 0.4239 - acc: 0.7923 - val_loss: 0.4322 - val_acc: 0.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.4235 - acc: 0.7869 - val_loss: 0.4315 - val_acc: 0.7815\n",
      "Epoch 114/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.4228 - acc: 0.7760 - val_loss: 0.4331 - val_acc: 0.7778\n",
      "Epoch 115/1000\n",
      "183/183 [==============================] - 0s 289us/step - loss: 0.4244 - acc: 0.7705 - val_loss: 0.4326 - val_acc: 0.7778\n",
      "Epoch 116/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4230 - acc: 0.7869 - val_loss: 0.4302 - val_acc: 0.7963\n",
      "Epoch 117/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.4265 - acc: 0.7760 - val_loss: 0.4301 - val_acc: 0.7963\n",
      "Epoch 118/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4259 - acc: 0.7760 - val_loss: 0.4441 - val_acc: 0.7704\n",
      "Epoch 119/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.4298 - acc: 0.7596 - val_loss: 0.4300 - val_acc: 0.7963\n",
      "Epoch 120/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.4263 - acc: 0.7869 - val_loss: 0.4329 - val_acc: 0.7815\n",
      "Epoch 121/1000\n",
      "183/183 [==============================] - 0s 314us/step - loss: 0.4223 - acc: 0.7705 - val_loss: 0.4317 - val_acc: 0.7778\n",
      "Epoch 122/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4228 - acc: 0.7814 - val_loss: 0.4316 - val_acc: 0.7778\n",
      "Epoch 123/1000\n",
      "183/183 [==============================] - 0s 341us/step - loss: 0.4205 - acc: 0.7978 - val_loss: 0.4294 - val_acc: 0.7963\n",
      "Epoch 124/1000\n",
      "183/183 [==============================] - 0s 386us/step - loss: 0.4232 - acc: 0.7869 - val_loss: 0.4309 - val_acc: 0.7741\n",
      "Epoch 125/1000\n",
      "183/183 [==============================] - 0s 294us/step - loss: 0.4216 - acc: 0.7760 - val_loss: 0.4288 - val_acc: 0.7852\n",
      "Epoch 126/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.4189 - acc: 0.7814 - val_loss: 0.4316 - val_acc: 0.7815\n",
      "Epoch 127/1000\n",
      "183/183 [==============================] - 0s 345us/step - loss: 0.4206 - acc: 0.7814 - val_loss: 0.4327 - val_acc: 0.7778\n",
      "Epoch 128/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.4215 - acc: 0.7869 - val_loss: 0.4306 - val_acc: 0.7963\n",
      "Epoch 129/1000\n",
      "183/183 [==============================] - 0s 351us/step - loss: 0.4194 - acc: 0.7978 - val_loss: 0.4319 - val_acc: 0.7741\n",
      "Epoch 130/1000\n",
      "183/183 [==============================] - 0s 289us/step - loss: 0.4191 - acc: 0.7760 - val_loss: 0.4323 - val_acc: 0.7815\n",
      "Epoch 131/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4197 - acc: 0.7923 - val_loss: 0.4325 - val_acc: 0.7963\n",
      "Epoch 132/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.4200 - acc: 0.7869 - val_loss: 0.4277 - val_acc: 0.7852\n",
      "Epoch 133/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4197 - acc: 0.7814 - val_loss: 0.4283 - val_acc: 0.7926\n",
      "Epoch 134/1000\n",
      "183/183 [==============================] - 0s 348us/step - loss: 0.4194 - acc: 0.7869 - val_loss: 0.4280 - val_acc: 0.7852\n",
      "Epoch 135/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.4213 - acc: 0.7869 - val_loss: 0.4311 - val_acc: 0.7778\n",
      "Epoch 136/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.4214 - acc: 0.7760 - val_loss: 0.4284 - val_acc: 0.7815\n",
      "Epoch 137/1000\n",
      "183/183 [==============================] - 0s 356us/step - loss: 0.4200 - acc: 0.7978 - val_loss: 0.4284 - val_acc: 0.7963\n",
      "Epoch 138/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.4165 - acc: 0.7814 - val_loss: 0.4308 - val_acc: 0.7852\n",
      "Epoch 139/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.4209 - acc: 0.7760 - val_loss: 0.4295 - val_acc: 0.7741\n",
      "Epoch 140/1000\n",
      "183/183 [==============================] - 0s 275us/step - loss: 0.4242 - acc: 0.7978 - val_loss: 0.4373 - val_acc: 0.7889\n",
      "Epoch 141/1000\n",
      "183/183 [==============================] - 0s 342us/step - loss: 0.4152 - acc: 0.8087 - val_loss: 0.4272 - val_acc: 0.7963\n",
      "Epoch 142/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4181 - acc: 0.7923 - val_loss: 0.4266 - val_acc: 0.7963\n",
      "Epoch 143/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.4167 - acc: 0.7869 - val_loss: 0.4292 - val_acc: 0.7889\n",
      "Epoch 144/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.4228 - acc: 0.7869 - val_loss: 0.4335 - val_acc: 0.7852\n",
      "Epoch 145/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.4185 - acc: 0.7978 - val_loss: 0.4289 - val_acc: 0.7926\n",
      "Epoch 146/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.4184 - acc: 0.7814 - val_loss: 0.4273 - val_acc: 0.7778\n",
      "Epoch 147/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.4279 - acc: 0.7814 - val_loss: 0.4351 - val_acc: 0.7889\n",
      "Epoch 148/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4254 - acc: 0.7814 - val_loss: 0.4257 - val_acc: 0.7963\n",
      "Epoch 149/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.4163 - acc: 0.7869 - val_loss: 0.4282 - val_acc: 0.7815\n",
      "Epoch 150/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4158 - acc: 0.7814 - val_loss: 0.4294 - val_acc: 0.7926\n",
      "Epoch 151/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.4135 - acc: 0.7923 - val_loss: 0.4272 - val_acc: 0.7815\n",
      "Epoch 152/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.4141 - acc: 0.7814 - val_loss: 0.4253 - val_acc: 0.7852\n",
      "Epoch 153/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.4143 - acc: 0.7923 - val_loss: 0.4252 - val_acc: 0.7926\n",
      "Epoch 154/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.4168 - acc: 0.7923 - val_loss: 0.4287 - val_acc: 0.7778\n",
      "Epoch 155/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.4127 - acc: 0.7869 - val_loss: 0.4251 - val_acc: 0.7889\n",
      "Epoch 156/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4140 - acc: 0.7869 - val_loss: 0.4247 - val_acc: 0.7815\n",
      "Epoch 157/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.4168 - acc: 0.7869 - val_loss: 0.4251 - val_acc: 0.7889\n",
      "Epoch 158/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.4136 - acc: 0.7760 - val_loss: 0.4306 - val_acc: 0.7926\n",
      "Epoch 159/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.4130 - acc: 0.8087 - val_loss: 0.4260 - val_acc: 0.7926\n",
      "Epoch 160/1000\n",
      "183/183 [==============================] - 0s 276us/step - loss: 0.4114 - acc: 0.7814 - val_loss: 0.4251 - val_acc: 0.7815\n",
      "Epoch 161/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.4128 - acc: 0.7978 - val_loss: 0.4259 - val_acc: 0.7889\n",
      "Epoch 162/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.4160 - acc: 0.7869 - val_loss: 0.4233 - val_acc: 0.7815\n",
      "Epoch 163/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.4147 - acc: 0.7814 - val_loss: 0.4331 - val_acc: 0.7778\n",
      "Epoch 164/1000\n",
      "183/183 [==============================] - 0s 254us/step - loss: 0.4144 - acc: 0.7923 - val_loss: 0.4229 - val_acc: 0.7963\n",
      "Epoch 165/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.4114 - acc: 0.7923 - val_loss: 0.4239 - val_acc: 0.7852\n",
      "Epoch 166/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.4109 - acc: 0.7978 - val_loss: 0.4250 - val_acc: 0.7815\n",
      "Epoch 167/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.4092 - acc: 0.7978 - val_loss: 0.4265 - val_acc: 0.7741\n",
      "Epoch 168/1000\n",
      "183/183 [==============================] - 0s 302us/step - loss: 0.4115 - acc: 0.7869 - val_loss: 0.4267 - val_acc: 0.7889\n",
      "Epoch 169/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.4115 - acc: 0.7869 - val_loss: 0.4243 - val_acc: 0.7852\n",
      "Epoch 170/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.4124 - acc: 0.8033 - val_loss: 0.4217 - val_acc: 0.7889\n",
      "Epoch 171/1000\n",
      "183/183 [==============================] - 0s 289us/step - loss: 0.4101 - acc: 0.7978 - val_loss: 0.4241 - val_acc: 0.7815\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - ETA: 0s - loss: 0.4309 - acc: 0.718 - 0s 238us/step - loss: 0.4111 - acc: 0.7814 - val_loss: 0.4247 - val_acc: 0.7815\n",
      "Epoch 173/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.4138 - acc: 0.7978 - val_loss: 0.4235 - val_acc: 0.7852\n",
      "Epoch 174/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4077 - acc: 0.7978 - val_loss: 0.4229 - val_acc: 0.7926\n",
      "Epoch 175/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.4145 - acc: 0.7869 - val_loss: 0.4237 - val_acc: 0.7852\n",
      "Epoch 176/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.4087 - acc: 0.7978 - val_loss: 0.4244 - val_acc: 0.7852\n",
      "Epoch 177/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.4067 - acc: 0.7978 - val_loss: 0.4303 - val_acc: 0.7852\n",
      "Epoch 178/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.4088 - acc: 0.7978 - val_loss: 0.4239 - val_acc: 0.7889\n",
      "Epoch 179/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.4079 - acc: 0.7978 - val_loss: 0.4215 - val_acc: 0.7889\n",
      "Epoch 180/1000\n",
      "183/183 [==============================] - 0s 230us/step - loss: 0.4088 - acc: 0.7978 - val_loss: 0.4239 - val_acc: 0.7926\n",
      "Epoch 181/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.4115 - acc: 0.7760 - val_loss: 0.4236 - val_acc: 0.7852\n",
      "Epoch 182/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.4068 - acc: 0.7978 - val_loss: 0.4206 - val_acc: 0.7926\n",
      "Epoch 183/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.4071 - acc: 0.8033 - val_loss: 0.4205 - val_acc: 0.7889\n",
      "Epoch 184/1000\n",
      "183/183 [==============================] - 0s 291us/step - loss: 0.4121 - acc: 0.7869 - val_loss: 0.4210 - val_acc: 0.7889\n",
      "Epoch 185/1000\n",
      "183/183 [==============================] - 0s 294us/step - loss: 0.4023 - acc: 0.8033 - val_loss: 0.4289 - val_acc: 0.7852\n",
      "Epoch 186/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.4076 - acc: 0.7978 - val_loss: 0.4277 - val_acc: 0.7889\n",
      "Epoch 187/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.4061 - acc: 0.7923 - val_loss: 0.4209 - val_acc: 0.7852\n",
      "Epoch 188/1000\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4595 - acc: 0.750 - 0s 255us/step - loss: 0.4053 - acc: 0.7923 - val_loss: 0.4232 - val_acc: 0.7852\n",
      "Epoch 189/1000\n",
      "183/183 [==============================] - 0s 276us/step - loss: 0.4081 - acc: 0.8087 - val_loss: 0.4237 - val_acc: 0.7815\n",
      "Epoch 190/1000\n",
      "183/183 [==============================] - 0s 291us/step - loss: 0.4031 - acc: 0.8033 - val_loss: 0.4219 - val_acc: 0.7889\n",
      "Epoch 191/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.4047 - acc: 0.7869 - val_loss: 0.4221 - val_acc: 0.7889\n",
      "Epoch 192/1000\n",
      "183/183 [==============================] - 0s 272us/step - loss: 0.4036 - acc: 0.7978 - val_loss: 0.4224 - val_acc: 0.7889\n",
      "Epoch 193/1000\n",
      "183/183 [==============================] - 0s 262us/step - loss: 0.4094 - acc: 0.7978 - val_loss: 0.4317 - val_acc: 0.7852\n",
      "Epoch 194/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.4004 - acc: 0.7978 - val_loss: 0.4197 - val_acc: 0.7815\n",
      "Epoch 195/1000\n",
      "183/183 [==============================] - 0s 295us/step - loss: 0.4088 - acc: 0.7923 - val_loss: 0.4213 - val_acc: 0.7926\n",
      "Epoch 196/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4022 - acc: 0.7923 - val_loss: 0.4225 - val_acc: 0.7852\n",
      "Epoch 197/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.4123 - acc: 0.7705 - val_loss: 0.4345 - val_acc: 0.7926\n",
      "Epoch 198/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4018 - acc: 0.8197 - val_loss: 0.4201 - val_acc: 0.7963\n",
      "Epoch 199/1000\n",
      "183/183 [==============================] - 0s 276us/step - loss: 0.4073 - acc: 0.7978 - val_loss: 0.4183 - val_acc: 0.7926\n",
      "Epoch 200/1000\n",
      "183/183 [==============================] - 0s 286us/step - loss: 0.4013 - acc: 0.8033 - val_loss: 0.4211 - val_acc: 0.7889\n",
      "Epoch 201/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.4089 - acc: 0.7923 - val_loss: 0.4278 - val_acc: 0.7815\n",
      "Epoch 202/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.4011 - acc: 0.7923 - val_loss: 0.4197 - val_acc: 0.7815\n",
      "Epoch 203/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.4045 - acc: 0.7978 - val_loss: 0.4203 - val_acc: 0.7889\n",
      "Epoch 204/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.4011 - acc: 0.7978 - val_loss: 0.4182 - val_acc: 0.7926\n",
      "Epoch 205/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.4010 - acc: 0.8087 - val_loss: 0.4229 - val_acc: 0.7926\n",
      "Epoch 206/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.3994 - acc: 0.8087 - val_loss: 0.4209 - val_acc: 0.7926\n",
      "Epoch 207/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.4011 - acc: 0.8033 - val_loss: 0.4189 - val_acc: 0.7852\n",
      "Epoch 208/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.4000 - acc: 0.7978 - val_loss: 0.4241 - val_acc: 0.7815\n",
      "Epoch 209/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.4003 - acc: 0.7978 - val_loss: 0.4183 - val_acc: 0.8000\n",
      "Epoch 210/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.4034 - acc: 0.7923 - val_loss: 0.4224 - val_acc: 0.7852\n",
      "Epoch 211/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3999 - acc: 0.7923 - val_loss: 0.4205 - val_acc: 0.7815\n",
      "Epoch 212/1000\n",
      "183/183 [==============================] - 0s 281us/step - loss: 0.3984 - acc: 0.7869 - val_loss: 0.4199 - val_acc: 0.7852\n",
      "Epoch 213/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.4065 - acc: 0.7814 - val_loss: 0.4245 - val_acc: 0.7889\n",
      "Epoch 214/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.4058 - acc: 0.7869 - val_loss: 0.4181 - val_acc: 0.8000\n",
      "Epoch 215/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.4012 - acc: 0.8142 - val_loss: 0.4201 - val_acc: 0.7889\n",
      "Epoch 216/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.3988 - acc: 0.8087 - val_loss: 0.4225 - val_acc: 0.7889\n",
      "Epoch 217/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.4000 - acc: 0.8033 - val_loss: 0.4178 - val_acc: 0.7852\n",
      "Epoch 218/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.4032 - acc: 0.7869 - val_loss: 0.4222 - val_acc: 0.7852\n",
      "Epoch 219/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3997 - acc: 0.7923 - val_loss: 0.4187 - val_acc: 0.7889\n",
      "Epoch 220/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3982 - acc: 0.7869 - val_loss: 0.4196 - val_acc: 0.7889\n",
      "Epoch 221/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3980 - acc: 0.7869 - val_loss: 0.4270 - val_acc: 0.7889\n",
      "Epoch 222/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3966 - acc: 0.8087 - val_loss: 0.4186 - val_acc: 0.7889\n",
      "Epoch 223/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.4012 - acc: 0.8033 - val_loss: 0.4183 - val_acc: 0.7889\n",
      "Epoch 224/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.3969 - acc: 0.7978 - val_loss: 0.4244 - val_acc: 0.7815\n",
      "Epoch 225/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3979 - acc: 0.8087 - val_loss: 0.4168 - val_acc: 0.7815\n",
      "Epoch 226/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3973 - acc: 0.8033 - val_loss: 0.4164 - val_acc: 0.7963\n",
      "Epoch 227/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.3945 - acc: 0.8033 - val_loss: 0.4220 - val_acc: 0.7852\n",
      "Epoch 228/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3967 - acc: 0.7978 - val_loss: 0.4200 - val_acc: 0.7889\n",
      "Epoch 229/1000\n",
      "183/183 [==============================] - 0s 235us/step - loss: 0.3960 - acc: 0.8087 - val_loss: 0.4192 - val_acc: 0.7926\n",
      "Epoch 230/1000\n",
      "183/183 [==============================] - 0s 227us/step - loss: 0.3957 - acc: 0.7978 - val_loss: 0.4200 - val_acc: 0.7852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3994 - acc: 0.7978 - val_loss: 0.4174 - val_acc: 0.7889\n",
      "Epoch 232/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.4046 - acc: 0.7869 - val_loss: 0.4280 - val_acc: 0.7963\n",
      "Epoch 233/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.3997 - acc: 0.8033 - val_loss: 0.4164 - val_acc: 0.8037\n",
      "Epoch 234/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3962 - acc: 0.7978 - val_loss: 0.4182 - val_acc: 0.7926\n",
      "Epoch 235/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.3944 - acc: 0.8087 - val_loss: 0.4180 - val_acc: 0.7926\n",
      "Epoch 236/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3940 - acc: 0.8033 - val_loss: 0.4171 - val_acc: 0.7926\n",
      "Epoch 237/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3925 - acc: 0.8087 - val_loss: 0.4196 - val_acc: 0.7926\n",
      "Epoch 238/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3955 - acc: 0.8087 - val_loss: 0.4248 - val_acc: 0.7815\n",
      "Epoch 239/1000\n",
      "183/183 [==============================] - 0s 340us/step - loss: 0.3945 - acc: 0.8142 - val_loss: 0.4162 - val_acc: 0.8000\n",
      "Epoch 240/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3940 - acc: 0.8087 - val_loss: 0.4182 - val_acc: 0.7963\n",
      "Epoch 241/1000\n",
      "183/183 [==============================] - 0s 337us/step - loss: 0.3926 - acc: 0.8087 - val_loss: 0.4180 - val_acc: 0.7963\n",
      "Epoch 242/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3935 - acc: 0.8033 - val_loss: 0.4156 - val_acc: 0.7889\n",
      "Epoch 243/1000\n",
      "183/183 [==============================] - 0s 370us/step - loss: 0.3928 - acc: 0.8087 - val_loss: 0.4190 - val_acc: 0.7963\n",
      "Epoch 244/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3953 - acc: 0.8087 - val_loss: 0.4234 - val_acc: 0.7852\n",
      "Epoch 245/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3918 - acc: 0.7978 - val_loss: 0.4165 - val_acc: 0.7926\n",
      "Epoch 246/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.4002 - acc: 0.7978 - val_loss: 0.4178 - val_acc: 0.7963\n",
      "Epoch 247/1000\n",
      "183/183 [==============================] - 0s 347us/step - loss: 0.3910 - acc: 0.8142 - val_loss: 0.4201 - val_acc: 0.7852\n",
      "Epoch 248/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.3918 - acc: 0.7978 - val_loss: 0.4190 - val_acc: 0.7889\n",
      "Epoch 249/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.3893 - acc: 0.8142 - val_loss: 0.4158 - val_acc: 0.7889\n",
      "Epoch 250/1000\n",
      "183/183 [==============================] - 0s 342us/step - loss: 0.3912 - acc: 0.8033 - val_loss: 0.4170 - val_acc: 0.7926\n",
      "Epoch 251/1000\n",
      "183/183 [==============================] - 0s 370us/step - loss: 0.3901 - acc: 0.8087 - val_loss: 0.4153 - val_acc: 0.8037\n",
      "Epoch 252/1000\n",
      "183/183 [==============================] - 0s 331us/step - loss: 0.3902 - acc: 0.8087 - val_loss: 0.4181 - val_acc: 0.7963\n",
      "Epoch 253/1000\n",
      "183/183 [==============================] - 0s 351us/step - loss: 0.3934 - acc: 0.7923 - val_loss: 0.4183 - val_acc: 0.7889\n",
      "Epoch 254/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.3929 - acc: 0.8087 - val_loss: 0.4160 - val_acc: 0.7889\n",
      "Epoch 255/1000\n",
      "183/183 [==============================] - 0s 348us/step - loss: 0.3897 - acc: 0.8033 - val_loss: 0.4214 - val_acc: 0.7852\n",
      "Epoch 256/1000\n",
      "183/183 [==============================] - 0s 381us/step - loss: 0.3894 - acc: 0.8087 - val_loss: 0.4214 - val_acc: 0.7852\n",
      "Epoch 257/1000\n",
      "183/183 [==============================] - 0s 367us/step - loss: 0.3887 - acc: 0.8033 - val_loss: 0.4153 - val_acc: 0.7926\n",
      "Epoch 258/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.3900 - acc: 0.8033 - val_loss: 0.4153 - val_acc: 0.7963\n",
      "Epoch 259/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3893 - acc: 0.7923 - val_loss: 0.4236 - val_acc: 0.7815\n",
      "Epoch 260/1000\n",
      "183/183 [==============================] - 0s 447us/step - loss: 0.3917 - acc: 0.7978 - val_loss: 0.4155 - val_acc: 0.7963\n",
      "Epoch 261/1000\n",
      "183/183 [==============================] - 0s 375us/step - loss: 0.3878 - acc: 0.8087 - val_loss: 0.4163 - val_acc: 0.7963\n",
      "Epoch 262/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3918 - acc: 0.8087 - val_loss: 0.4138 - val_acc: 0.8037\n",
      "Epoch 263/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3889 - acc: 0.8087 - val_loss: 0.4183 - val_acc: 0.7926\n",
      "Epoch 264/1000\n",
      "183/183 [==============================] - 0s 311us/step - loss: 0.3915 - acc: 0.8087 - val_loss: 0.4157 - val_acc: 0.7963\n",
      "Epoch 265/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3902 - acc: 0.8087 - val_loss: 0.4201 - val_acc: 0.7889\n",
      "Epoch 266/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3991 - acc: 0.8033 - val_loss: 0.4138 - val_acc: 0.8074\n",
      "Epoch 267/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3898 - acc: 0.7978 - val_loss: 0.4198 - val_acc: 0.7815\n",
      "Epoch 268/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3881 - acc: 0.7923 - val_loss: 0.4152 - val_acc: 0.7926\n",
      "Epoch 269/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3864 - acc: 0.8033 - val_loss: 0.4144 - val_acc: 0.8037\n",
      "Epoch 270/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.3873 - acc: 0.8197 - val_loss: 0.4191 - val_acc: 0.7889\n",
      "Epoch 271/1000\n",
      "183/183 [==============================] - 0s 335us/step - loss: 0.3875 - acc: 0.8033 - val_loss: 0.4184 - val_acc: 0.7926\n",
      "Epoch 272/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3902 - acc: 0.8033 - val_loss: 0.4188 - val_acc: 0.7926\n",
      "Epoch 273/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3917 - acc: 0.7978 - val_loss: 0.4142 - val_acc: 0.8000\n",
      "Epoch 274/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3883 - acc: 0.8142 - val_loss: 0.4153 - val_acc: 0.7963\n",
      "Epoch 275/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3888 - acc: 0.8033 - val_loss: 0.4214 - val_acc: 0.7852\n",
      "Epoch 276/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3860 - acc: 0.8033 - val_loss: 0.4136 - val_acc: 0.7963\n",
      "Epoch 277/1000\n",
      "183/183 [==============================] - 0s 369us/step - loss: 0.3862 - acc: 0.8087 - val_loss: 0.4143 - val_acc: 0.7926\n",
      "Epoch 278/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.3855 - acc: 0.8033 - val_loss: 0.4193 - val_acc: 0.7852\n",
      "Epoch 279/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.3850 - acc: 0.8033 - val_loss: 0.4201 - val_acc: 0.7852\n",
      "Epoch 280/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3846 - acc: 0.8087 - val_loss: 0.4142 - val_acc: 0.7963\n",
      "Epoch 281/1000\n",
      "183/183 [==============================] - 0s 311us/step - loss: 0.3879 - acc: 0.8087 - val_loss: 0.4178 - val_acc: 0.7926\n",
      "Epoch 282/1000\n",
      "183/183 [==============================] - 0s 353us/step - loss: 0.3832 - acc: 0.8087 - val_loss: 0.4156 - val_acc: 0.7926\n",
      "Epoch 283/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3835 - acc: 0.8033 - val_loss: 0.4152 - val_acc: 0.8037\n",
      "Epoch 284/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3837 - acc: 0.8142 - val_loss: 0.4142 - val_acc: 0.7963\n",
      "Epoch 285/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3874 - acc: 0.7978 - val_loss: 0.4201 - val_acc: 0.7889\n",
      "Epoch 286/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3870 - acc: 0.8197 - val_loss: 0.4134 - val_acc: 0.8074\n",
      "Epoch 287/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3844 - acc: 0.8142 - val_loss: 0.4138 - val_acc: 0.7963\n",
      "Epoch 288/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.3850 - acc: 0.8087 - val_loss: 0.4210 - val_acc: 0.7852\n",
      "Epoch 289/1000\n",
      "183/183 [==============================] - 0s 367us/step - loss: 0.3832 - acc: 0.8197 - val_loss: 0.4180 - val_acc: 0.7926\n",
      "Epoch 290/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 298us/step - loss: 0.3799 - acc: 0.8087 - val_loss: 0.4132 - val_acc: 0.8037\n",
      "Epoch 291/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3839 - acc: 0.8142 - val_loss: 0.4134 - val_acc: 0.8037\n",
      "Epoch 292/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3814 - acc: 0.8142 - val_loss: 0.4185 - val_acc: 0.7926\n",
      "Epoch 293/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3818 - acc: 0.8087 - val_loss: 0.4174 - val_acc: 0.7852\n",
      "Epoch 294/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3810 - acc: 0.8033 - val_loss: 0.4145 - val_acc: 0.7963\n",
      "Epoch 295/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.3826 - acc: 0.8087 - val_loss: 0.4129 - val_acc: 0.8037\n",
      "Epoch 296/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3901 - acc: 0.8033 - val_loss: 0.4197 - val_acc: 0.7852\n",
      "Epoch 297/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3816 - acc: 0.7978 - val_loss: 0.4137 - val_acc: 0.8037\n",
      "Epoch 298/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3813 - acc: 0.8087 - val_loss: 0.4142 - val_acc: 0.8000\n",
      "Epoch 299/1000\n",
      "183/183 [==============================] - 0s 332us/step - loss: 0.3801 - acc: 0.8142 - val_loss: 0.4150 - val_acc: 0.7889\n",
      "Epoch 300/1000\n",
      "183/183 [==============================] - 0s 332us/step - loss: 0.4014 - acc: 0.7650 - val_loss: 0.4263 - val_acc: 0.7963\n",
      "Epoch 301/1000\n",
      "183/183 [==============================] - 0s 326us/step - loss: 0.3786 - acc: 0.8197 - val_loss: 0.4132 - val_acc: 0.7963\n",
      "Epoch 302/1000\n",
      "183/183 [==============================] - 0s 321us/step - loss: 0.3884 - acc: 0.7978 - val_loss: 0.4129 - val_acc: 0.8037\n",
      "Epoch 303/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3854 - acc: 0.8251 - val_loss: 0.4261 - val_acc: 0.8000\n",
      "Epoch 304/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.3888 - acc: 0.8033 - val_loss: 0.4112 - val_acc: 0.8000\n",
      "Epoch 305/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3792 - acc: 0.8033 - val_loss: 0.4145 - val_acc: 0.7963\n",
      "Epoch 306/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3781 - acc: 0.8087 - val_loss: 0.4149 - val_acc: 0.7963\n",
      "Epoch 307/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3776 - acc: 0.8087 - val_loss: 0.4144 - val_acc: 0.8000\n",
      "Epoch 308/1000\n",
      "183/183 [==============================] - 0s 264us/step - loss: 0.3811 - acc: 0.8087 - val_loss: 0.4148 - val_acc: 0.7926\n",
      "Epoch 309/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.3765 - acc: 0.8033 - val_loss: 0.4125 - val_acc: 0.8000\n",
      "Epoch 310/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.3804 - acc: 0.8142 - val_loss: 0.4124 - val_acc: 0.8000\n",
      "Epoch 311/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3857 - acc: 0.7869 - val_loss: 0.4184 - val_acc: 0.7889\n",
      "Epoch 312/1000\n",
      "183/183 [==============================] - 0s 295us/step - loss: 0.3917 - acc: 0.8087 - val_loss: 0.4114 - val_acc: 0.8037\n",
      "Epoch 313/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.3828 - acc: 0.8087 - val_loss: 0.4217 - val_acc: 0.8000\n",
      "Epoch 314/1000\n",
      "183/183 [==============================] - 0s 273us/step - loss: 0.3837 - acc: 0.8251 - val_loss: 0.4180 - val_acc: 0.7889\n",
      "Epoch 315/1000\n",
      "183/183 [==============================] - 0s 291us/step - loss: 0.3768 - acc: 0.7978 - val_loss: 0.4130 - val_acc: 0.8037\n",
      "Epoch 316/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.3845 - acc: 0.8033 - val_loss: 0.4128 - val_acc: 0.8000\n",
      "Epoch 317/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3751 - acc: 0.8087 - val_loss: 0.4140 - val_acc: 0.7963\n",
      "Epoch 318/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.3759 - acc: 0.8087 - val_loss: 0.4145 - val_acc: 0.7963\n",
      "Epoch 319/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3742 - acc: 0.8033 - val_loss: 0.4140 - val_acc: 0.8000\n",
      "Epoch 320/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.3749 - acc: 0.8033 - val_loss: 0.4131 - val_acc: 0.7963\n",
      "Epoch 321/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3857 - acc: 0.8033 - val_loss: 0.4169 - val_acc: 0.7926\n",
      "Epoch 322/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.3781 - acc: 0.8033 - val_loss: 0.4118 - val_acc: 0.8037\n",
      "Epoch 323/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3759 - acc: 0.8142 - val_loss: 0.4109 - val_acc: 0.8000\n",
      "Epoch 324/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.3741 - acc: 0.8033 - val_loss: 0.4204 - val_acc: 0.7889\n",
      "Epoch 325/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.3759 - acc: 0.8197 - val_loss: 0.4137 - val_acc: 0.8000\n",
      "Epoch 326/1000\n",
      "183/183 [==============================] - 0s 322us/step - loss: 0.3745 - acc: 0.8033 - val_loss: 0.4113 - val_acc: 0.8000\n",
      "Epoch 327/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.3742 - acc: 0.8087 - val_loss: 0.4144 - val_acc: 0.7926\n",
      "Epoch 328/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.3730 - acc: 0.8087 - val_loss: 0.4131 - val_acc: 0.8000\n",
      "Epoch 329/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.3733 - acc: 0.8087 - val_loss: 0.4121 - val_acc: 0.7963\n",
      "Epoch 330/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.3772 - acc: 0.7978 - val_loss: 0.4163 - val_acc: 0.7926\n",
      "Epoch 331/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3779 - acc: 0.8033 - val_loss: 0.4109 - val_acc: 0.8000\n",
      "Epoch 332/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.3718 - acc: 0.8087 - val_loss: 0.4156 - val_acc: 0.7852\n",
      "Epoch 333/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.3723 - acc: 0.8142 - val_loss: 0.4197 - val_acc: 0.8000\n",
      "Epoch 334/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.3737 - acc: 0.8142 - val_loss: 0.4132 - val_acc: 0.8000\n",
      "Epoch 335/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3715 - acc: 0.8142 - val_loss: 0.4115 - val_acc: 0.8037\n",
      "Epoch 336/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.3733 - acc: 0.8033 - val_loss: 0.4135 - val_acc: 0.7889\n",
      "Epoch 337/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3721 - acc: 0.8033 - val_loss: 0.4124 - val_acc: 0.7963\n",
      "Epoch 338/1000\n",
      "183/183 [==============================] - 0s 303us/step - loss: 0.3696 - acc: 0.8087 - val_loss: 0.4119 - val_acc: 0.7963\n",
      "Epoch 339/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.3710 - acc: 0.8033 - val_loss: 0.4128 - val_acc: 0.8000\n",
      "Epoch 340/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3712 - acc: 0.8142 - val_loss: 0.4156 - val_acc: 0.8074\n",
      "Epoch 341/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.3699 - acc: 0.8087 - val_loss: 0.4124 - val_acc: 0.7926\n",
      "Epoch 342/1000\n",
      "183/183 [==============================] - 0s 246us/step - loss: 0.3709 - acc: 0.8087 - val_loss: 0.4104 - val_acc: 0.8000\n",
      "Epoch 343/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3739 - acc: 0.8142 - val_loss: 0.4114 - val_acc: 0.8000\n",
      "Epoch 344/1000\n",
      "183/183 [==============================] - 0s 253us/step - loss: 0.3757 - acc: 0.8142 - val_loss: 0.4228 - val_acc: 0.8037\n",
      "Epoch 345/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.3709 - acc: 0.8142 - val_loss: 0.4115 - val_acc: 0.8074\n",
      "Epoch 346/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.3697 - acc: 0.8142 - val_loss: 0.4096 - val_acc: 0.8037\n",
      "Epoch 347/1000\n",
      "183/183 [==============================] - 0s 264us/step - loss: 0.3694 - acc: 0.8142 - val_loss: 0.4148 - val_acc: 0.7852\n",
      "Epoch 348/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3721 - acc: 0.8033 - val_loss: 0.4141 - val_acc: 0.7889\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 255us/step - loss: 0.3729 - acc: 0.8087 - val_loss: 0.4110 - val_acc: 0.7963\n",
      "Epoch 350/1000\n",
      "183/183 [==============================] - 0s 291us/step - loss: 0.3678 - acc: 0.8087 - val_loss: 0.4141 - val_acc: 0.7926\n",
      "Epoch 351/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.3689 - acc: 0.8197 - val_loss: 0.4203 - val_acc: 0.8000\n",
      "Epoch 352/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.3671 - acc: 0.8251 - val_loss: 0.4111 - val_acc: 0.8037\n",
      "Epoch 353/1000\n",
      "183/183 [==============================] - 0s 326us/step - loss: 0.3708 - acc: 0.8087 - val_loss: 0.4090 - val_acc: 0.8000\n",
      "Epoch 354/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.3697 - acc: 0.8087 - val_loss: 0.4175 - val_acc: 0.7926\n",
      "Epoch 355/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3716 - acc: 0.8142 - val_loss: 0.4105 - val_acc: 0.7963\n",
      "Epoch 356/1000\n",
      "183/183 [==============================] - 0s 321us/step - loss: 0.3665 - acc: 0.8087 - val_loss: 0.4119 - val_acc: 0.7963\n",
      "Epoch 357/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3659 - acc: 0.8033 - val_loss: 0.4125 - val_acc: 0.7926\n",
      "Epoch 358/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3661 - acc: 0.8033 - val_loss: 0.4128 - val_acc: 0.7926\n",
      "Epoch 359/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3666 - acc: 0.7923 - val_loss: 0.4103 - val_acc: 0.7963\n",
      "Epoch 360/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3650 - acc: 0.8142 - val_loss: 0.4099 - val_acc: 0.8000\n",
      "Epoch 361/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.3649 - acc: 0.8142 - val_loss: 0.4115 - val_acc: 0.7926\n",
      "Epoch 362/1000\n",
      "183/183 [==============================] - 0s 345us/step - loss: 0.3676 - acc: 0.8087 - val_loss: 0.4186 - val_acc: 0.7926\n",
      "Epoch 363/1000\n",
      "183/183 [==============================] - 0s 254us/step - loss: 0.3646 - acc: 0.8033 - val_loss: 0.4105 - val_acc: 0.8037\n",
      "Epoch 364/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3663 - acc: 0.8087 - val_loss: 0.4110 - val_acc: 0.7963\n",
      "Epoch 365/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3655 - acc: 0.8033 - val_loss: 0.4142 - val_acc: 0.8000\n",
      "Epoch 366/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3629 - acc: 0.8087 - val_loss: 0.4114 - val_acc: 0.7926\n",
      "Epoch 367/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3644 - acc: 0.8142 - val_loss: 0.4090 - val_acc: 0.7963\n",
      "Epoch 368/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3658 - acc: 0.8033 - val_loss: 0.4118 - val_acc: 0.7889\n",
      "Epoch 369/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.3630 - acc: 0.8087 - val_loss: 0.4092 - val_acc: 0.8000\n",
      "Epoch 370/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3675 - acc: 0.8033 - val_loss: 0.4141 - val_acc: 0.8000\n",
      "Epoch 371/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.3623 - acc: 0.7978 - val_loss: 0.4098 - val_acc: 0.7926\n",
      "Epoch 372/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3662 - acc: 0.8087 - val_loss: 0.4127 - val_acc: 0.7815\n",
      "Epoch 373/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3627 - acc: 0.8033 - val_loss: 0.4112 - val_acc: 0.7926\n",
      "Epoch 374/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3623 - acc: 0.8142 - val_loss: 0.4152 - val_acc: 0.7963\n",
      "Epoch 375/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3634 - acc: 0.8087 - val_loss: 0.4106 - val_acc: 0.7926\n",
      "Epoch 376/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.3619 - acc: 0.8087 - val_loss: 0.4110 - val_acc: 0.7926\n",
      "Epoch 377/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3614 - acc: 0.8087 - val_loss: 0.4168 - val_acc: 0.7926\n",
      "Epoch 378/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3644 - acc: 0.8197 - val_loss: 0.4098 - val_acc: 0.7963\n",
      "Epoch 379/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3600 - acc: 0.8142 - val_loss: 0.4137 - val_acc: 0.8000\n",
      "Epoch 380/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3645 - acc: 0.8251 - val_loss: 0.4139 - val_acc: 0.8000\n",
      "Epoch 381/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.3637 - acc: 0.8033 - val_loss: 0.4099 - val_acc: 0.7963\n",
      "Epoch 382/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.3605 - acc: 0.8142 - val_loss: 0.4085 - val_acc: 0.8000\n",
      "Epoch 383/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3584 - acc: 0.8087 - val_loss: 0.4135 - val_acc: 0.7963\n",
      "Epoch 384/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3640 - acc: 0.8251 - val_loss: 0.4173 - val_acc: 0.8000\n",
      "Epoch 385/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3579 - acc: 0.8142 - val_loss: 0.4086 - val_acc: 0.7926\n",
      "Epoch 386/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.3626 - acc: 0.8142 - val_loss: 0.4092 - val_acc: 0.7963\n",
      "Epoch 387/1000\n",
      "183/183 [==============================] - 0s 272us/step - loss: 0.3631 - acc: 0.8087 - val_loss: 0.4072 - val_acc: 0.8000\n",
      "Epoch 388/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.3566 - acc: 0.8087 - val_loss: 0.4099 - val_acc: 0.8000\n",
      "Epoch 389/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.3728 - acc: 0.7978 - val_loss: 0.4265 - val_acc: 0.7889\n",
      "Epoch 390/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.3597 - acc: 0.8197 - val_loss: 0.4092 - val_acc: 0.7889\n",
      "Epoch 391/1000\n",
      "183/183 [==============================] - 0s 283us/step - loss: 0.3650 - acc: 0.8087 - val_loss: 0.4109 - val_acc: 0.7926\n",
      "Epoch 392/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3592 - acc: 0.8197 - val_loss: 0.4139 - val_acc: 0.8000\n",
      "Epoch 393/1000\n",
      "183/183 [==============================] - 0s 289us/step - loss: 0.3567 - acc: 0.8197 - val_loss: 0.4128 - val_acc: 0.8000\n",
      "Epoch 394/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3557 - acc: 0.8197 - val_loss: 0.4108 - val_acc: 0.8074\n",
      "Epoch 395/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.3559 - acc: 0.8142 - val_loss: 0.4090 - val_acc: 0.7963\n",
      "Epoch 396/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.3574 - acc: 0.8033 - val_loss: 0.4119 - val_acc: 0.8000\n",
      "Epoch 397/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3550 - acc: 0.8087 - val_loss: 0.4100 - val_acc: 0.8037\n",
      "Epoch 398/1000\n",
      "183/183 [==============================] - 0s 331us/step - loss: 0.3541 - acc: 0.8306 - val_loss: 0.4103 - val_acc: 0.8000\n",
      "Epoch 399/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3582 - acc: 0.8033 - val_loss: 0.4073 - val_acc: 0.7963\n",
      "Epoch 400/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3571 - acc: 0.8251 - val_loss: 0.4160 - val_acc: 0.8111\n",
      "Epoch 401/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3560 - acc: 0.8306 - val_loss: 0.4106 - val_acc: 0.8037\n",
      "Epoch 402/1000\n",
      "183/183 [==============================] - 0s 375us/step - loss: 0.3594 - acc: 0.7978 - val_loss: 0.4087 - val_acc: 0.7963\n",
      "Epoch 403/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.3551 - acc: 0.8087 - val_loss: 0.4123 - val_acc: 0.8037\n",
      "Epoch 404/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3547 - acc: 0.8197 - val_loss: 0.4121 - val_acc: 0.8000\n",
      "Epoch 405/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.3608 - acc: 0.8033 - val_loss: 0.4069 - val_acc: 0.7926\n",
      "Epoch 406/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3635 - acc: 0.8251 - val_loss: 0.4160 - val_acc: 0.8074\n",
      "Epoch 407/1000\n",
      "183/183 [==============================] - 0s 336us/step - loss: 0.3559 - acc: 0.8251 - val_loss: 0.4099 - val_acc: 0.8037\n",
      "Epoch 408/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 309us/step - loss: 0.3509 - acc: 0.8142 - val_loss: 0.4073 - val_acc: 0.8000\n",
      "Epoch 409/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.3560 - acc: 0.8142 - val_loss: 0.4067 - val_acc: 0.7926\n",
      "Epoch 410/1000\n",
      "183/183 [==============================] - 0s 272us/step - loss: 0.3570 - acc: 0.8197 - val_loss: 0.4210 - val_acc: 0.8000\n",
      "Epoch 411/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3549 - acc: 0.8251 - val_loss: 0.4095 - val_acc: 0.8037\n",
      "Epoch 412/1000\n",
      "183/183 [==============================] - 0s 326us/step - loss: 0.3515 - acc: 0.8142 - val_loss: 0.4079 - val_acc: 0.7926\n",
      "Epoch 413/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3547 - acc: 0.8033 - val_loss: 0.4081 - val_acc: 0.8000\n",
      "Epoch 414/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3506 - acc: 0.8306 - val_loss: 0.4102 - val_acc: 0.8037\n",
      "Epoch 415/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3503 - acc: 0.8251 - val_loss: 0.4105 - val_acc: 0.8111\n",
      "Epoch 416/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3491 - acc: 0.8306 - val_loss: 0.4072 - val_acc: 0.8111\n",
      "Epoch 417/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3516 - acc: 0.8087 - val_loss: 0.4070 - val_acc: 0.7889\n",
      "Epoch 418/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3520 - acc: 0.8197 - val_loss: 0.4094 - val_acc: 0.8111\n",
      "Epoch 419/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3486 - acc: 0.8306 - val_loss: 0.4085 - val_acc: 0.8111\n",
      "Epoch 420/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3491 - acc: 0.8306 - val_loss: 0.4088 - val_acc: 0.8074\n",
      "Epoch 421/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3486 - acc: 0.8197 - val_loss: 0.4061 - val_acc: 0.7889\n",
      "Epoch 422/1000\n",
      "183/183 [==============================] - 0s 345us/step - loss: 0.3487 - acc: 0.8142 - val_loss: 0.4099 - val_acc: 0.8074\n",
      "Epoch 423/1000\n",
      "183/183 [==============================] - 0s 364us/step - loss: 0.3491 - acc: 0.8306 - val_loss: 0.4072 - val_acc: 0.8111\n",
      "Epoch 424/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3473 - acc: 0.8306 - val_loss: 0.4092 - val_acc: 0.8148\n",
      "Epoch 425/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.3505 - acc: 0.8361 - val_loss: 0.4108 - val_acc: 0.8074\n",
      "Epoch 426/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.3483 - acc: 0.8087 - val_loss: 0.4070 - val_acc: 0.7963\n",
      "Epoch 427/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3487 - acc: 0.8197 - val_loss: 0.4082 - val_acc: 0.8148\n",
      "Epoch 428/1000\n",
      "183/183 [==============================] - 0s 347us/step - loss: 0.3490 - acc: 0.8142 - val_loss: 0.4093 - val_acc: 0.8111\n",
      "Epoch 429/1000\n",
      "183/183 [==============================] - 0s 323us/step - loss: 0.3433 - acc: 0.8142 - val_loss: 0.4043 - val_acc: 0.8000\n",
      "Epoch 430/1000\n",
      "183/183 [==============================] - 0s 322us/step - loss: 0.3471 - acc: 0.8033 - val_loss: 0.4069 - val_acc: 0.7963\n",
      "Epoch 431/1000\n",
      "183/183 [==============================] - 0s 344us/step - loss: 0.3465 - acc: 0.8087 - val_loss: 0.4073 - val_acc: 0.8037\n",
      "Epoch 432/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3468 - acc: 0.8142 - val_loss: 0.4184 - val_acc: 0.8000\n",
      "Epoch 433/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.3463 - acc: 0.8251 - val_loss: 0.4068 - val_acc: 0.8000\n",
      "Epoch 434/1000\n",
      "183/183 [==============================] - 0s 281us/step - loss: 0.3457 - acc: 0.8251 - val_loss: 0.4053 - val_acc: 0.8000\n",
      "Epoch 435/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.3461 - acc: 0.8251 - val_loss: 0.4074 - val_acc: 0.8111\n",
      "Epoch 436/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.3425 - acc: 0.8361 - val_loss: 0.4100 - val_acc: 0.8074\n",
      "Epoch 437/1000\n",
      "183/183 [==============================] - 0s 267us/step - loss: 0.3437 - acc: 0.8361 - val_loss: 0.4098 - val_acc: 0.8074\n",
      "Epoch 438/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3463 - acc: 0.8251 - val_loss: 0.4070 - val_acc: 0.8074\n",
      "Epoch 439/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3423 - acc: 0.8306 - val_loss: 0.4080 - val_acc: 0.8148\n",
      "Epoch 440/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3414 - acc: 0.8361 - val_loss: 0.4069 - val_acc: 0.8185\n",
      "Epoch 441/1000\n",
      "183/183 [==============================] - 0s 329us/step - loss: 0.3455 - acc: 0.8306 - val_loss: 0.4103 - val_acc: 0.8148\n",
      "Epoch 442/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3496 - acc: 0.8197 - val_loss: 0.4036 - val_acc: 0.8037\n",
      "Epoch 443/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3436 - acc: 0.8251 - val_loss: 0.4091 - val_acc: 0.8185\n",
      "Epoch 444/1000\n",
      "183/183 [==============================] - 0s 321us/step - loss: 0.3421 - acc: 0.8415 - val_loss: 0.4066 - val_acc: 0.8074\n",
      "Epoch 445/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3436 - acc: 0.8142 - val_loss: 0.4052 - val_acc: 0.8074\n",
      "Epoch 446/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3413 - acc: 0.8306 - val_loss: 0.4161 - val_acc: 0.8000\n",
      "Epoch 447/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3423 - acc: 0.8197 - val_loss: 0.4077 - val_acc: 0.8074\n",
      "Epoch 448/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3385 - acc: 0.8306 - val_loss: 0.4032 - val_acc: 0.8000\n",
      "Epoch 449/1000\n",
      "183/183 [==============================] - 0s 334us/step - loss: 0.3407 - acc: 0.8142 - val_loss: 0.4026 - val_acc: 0.8111\n",
      "Epoch 450/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3371 - acc: 0.8361 - val_loss: 0.4086 - val_acc: 0.8074\n",
      "Epoch 451/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3417 - acc: 0.8251 - val_loss: 0.4104 - val_acc: 0.8037\n",
      "Epoch 452/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3467 - acc: 0.8087 - val_loss: 0.4058 - val_acc: 0.8000\n",
      "Epoch 453/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.3433 - acc: 0.8197 - val_loss: 0.4084 - val_acc: 0.8148\n",
      "Epoch 454/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3417 - acc: 0.8251 - val_loss: 0.4044 - val_acc: 0.8111\n",
      "Epoch 455/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.3374 - acc: 0.8306 - val_loss: 0.4046 - val_acc: 0.8074\n",
      "Epoch 456/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3373 - acc: 0.8251 - val_loss: 0.4030 - val_acc: 0.8111\n",
      "Epoch 457/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3352 - acc: 0.8306 - val_loss: 0.4074 - val_acc: 0.8148\n",
      "Epoch 458/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.3402 - acc: 0.8251 - val_loss: 0.4092 - val_acc: 0.8111\n",
      "Epoch 459/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3346 - acc: 0.8251 - val_loss: 0.4022 - val_acc: 0.8111\n",
      "Epoch 460/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3377 - acc: 0.8197 - val_loss: 0.4027 - val_acc: 0.8148\n",
      "Epoch 461/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3365 - acc: 0.8251 - val_loss: 0.4107 - val_acc: 0.8037\n",
      "Epoch 462/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.3360 - acc: 0.8306 - val_loss: 0.4026 - val_acc: 0.8148\n",
      "Epoch 463/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.3334 - acc: 0.8415 - val_loss: 0.4022 - val_acc: 0.8148\n",
      "Epoch 464/1000\n",
      "183/183 [==============================] - 0s 272us/step - loss: 0.3380 - acc: 0.8142 - val_loss: 0.4021 - val_acc: 0.8111\n",
      "Epoch 465/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.3305 - acc: 0.8361 - val_loss: 0.4086 - val_acc: 0.8074\n",
      "Epoch 466/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.3343 - acc: 0.8361 - val_loss: 0.4082 - val_acc: 0.8111\n",
      "Epoch 467/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 277us/step - loss: 0.3355 - acc: 0.8361 - val_loss: 0.4047 - val_acc: 0.8185\n",
      "Epoch 468/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.3316 - acc: 0.8361 - val_loss: 0.4065 - val_acc: 0.8148\n",
      "Epoch 469/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3350 - acc: 0.8470 - val_loss: 0.4040 - val_acc: 0.8185\n",
      "Epoch 470/1000\n",
      "183/183 [==============================] - 0s 232us/step - loss: 0.3326 - acc: 0.8361 - val_loss: 0.4000 - val_acc: 0.8037\n",
      "Epoch 471/1000\n",
      "183/183 [==============================] - 0s 291us/step - loss: 0.3319 - acc: 0.8306 - val_loss: 0.4025 - val_acc: 0.8148\n",
      "Epoch 472/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3312 - acc: 0.8306 - val_loss: 0.4124 - val_acc: 0.7963\n",
      "Epoch 473/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3330 - acc: 0.8306 - val_loss: 0.4035 - val_acc: 0.8185\n",
      "Epoch 474/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3307 - acc: 0.8415 - val_loss: 0.4039 - val_acc: 0.8222\n",
      "Epoch 475/1000\n",
      "183/183 [==============================] - 0s 305us/step - loss: 0.3314 - acc: 0.8306 - val_loss: 0.4034 - val_acc: 0.8185\n",
      "Epoch 476/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3283 - acc: 0.8361 - val_loss: 0.4012 - val_acc: 0.8185\n",
      "Epoch 477/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3308 - acc: 0.8415 - val_loss: 0.4034 - val_acc: 0.8111\n",
      "Epoch 478/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3286 - acc: 0.8415 - val_loss: 0.4035 - val_acc: 0.8185\n",
      "Epoch 479/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3280 - acc: 0.8361 - val_loss: 0.4019 - val_acc: 0.8222\n",
      "Epoch 480/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3279 - acc: 0.8415 - val_loss: 0.4015 - val_acc: 0.8185\n",
      "Epoch 481/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.3277 - acc: 0.8415 - val_loss: 0.4018 - val_acc: 0.8185\n",
      "Epoch 482/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3307 - acc: 0.8415 - val_loss: 0.4072 - val_acc: 0.8111\n",
      "Epoch 483/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3289 - acc: 0.8306 - val_loss: 0.4033 - val_acc: 0.8222\n",
      "Epoch 484/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3328 - acc: 0.8251 - val_loss: 0.4000 - val_acc: 0.8148\n",
      "Epoch 485/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3270 - acc: 0.8306 - val_loss: 0.4045 - val_acc: 0.8222\n",
      "Epoch 486/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.3289 - acc: 0.8361 - val_loss: 0.4031 - val_acc: 0.8222\n",
      "Epoch 487/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3269 - acc: 0.8415 - val_loss: 0.4003 - val_acc: 0.8111\n",
      "Epoch 488/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3239 - acc: 0.8415 - val_loss: 0.4047 - val_acc: 0.8111\n",
      "Epoch 489/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3266 - acc: 0.8361 - val_loss: 0.4028 - val_acc: 0.8259\n",
      "Epoch 490/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3243 - acc: 0.8470 - val_loss: 0.4025 - val_acc: 0.8259\n",
      "Epoch 491/1000\n",
      "183/183 [==============================] - 0s 326us/step - loss: 0.3239 - acc: 0.8415 - val_loss: 0.4012 - val_acc: 0.8185\n",
      "Epoch 492/1000\n",
      "183/183 [==============================] - 0s 254us/step - loss: 0.3233 - acc: 0.8415 - val_loss: 0.4012 - val_acc: 0.8185\n",
      "Epoch 493/1000\n",
      "183/183 [==============================] - 0s 292us/step - loss: 0.3241 - acc: 0.8415 - val_loss: 0.3994 - val_acc: 0.8185\n",
      "Epoch 494/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3238 - acc: 0.8470 - val_loss: 0.4042 - val_acc: 0.8185\n",
      "Epoch 495/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3230 - acc: 0.8415 - val_loss: 0.3999 - val_acc: 0.8222\n",
      "Epoch 496/1000\n",
      "183/183 [==============================] - 0s 329us/step - loss: 0.3214 - acc: 0.8415 - val_loss: 0.4013 - val_acc: 0.8185\n",
      "Epoch 497/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.3257 - acc: 0.8361 - val_loss: 0.4028 - val_acc: 0.8185\n",
      "Epoch 498/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3275 - acc: 0.8361 - val_loss: 0.3982 - val_acc: 0.8148\n",
      "Epoch 499/1000\n",
      "183/183 [==============================] - 0s 362us/step - loss: 0.3222 - acc: 0.8415 - val_loss: 0.3982 - val_acc: 0.8222\n",
      "Epoch 500/1000\n",
      "183/183 [==============================] - 0s 340us/step - loss: 0.3282 - acc: 0.8251 - val_loss: 0.4090 - val_acc: 0.8074\n",
      "Epoch 501/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3219 - acc: 0.8251 - val_loss: 0.3987 - val_acc: 0.8259\n",
      "Epoch 502/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.3218 - acc: 0.8470 - val_loss: 0.4003 - val_acc: 0.8222\n",
      "Epoch 503/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3266 - acc: 0.8197 - val_loss: 0.4076 - val_acc: 0.8074\n",
      "Epoch 504/1000\n",
      "183/183 [==============================] - 0s 314us/step - loss: 0.3205 - acc: 0.8306 - val_loss: 0.3997 - val_acc: 0.8185\n",
      "Epoch 505/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.3216 - acc: 0.8361 - val_loss: 0.4006 - val_acc: 0.8148\n",
      "Epoch 506/1000\n",
      "183/183 [==============================] - 0s 324us/step - loss: 0.3180 - acc: 0.8525 - val_loss: 0.4004 - val_acc: 0.8185\n",
      "Epoch 507/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3201 - acc: 0.8525 - val_loss: 0.3980 - val_acc: 0.8296\n",
      "Epoch 508/1000\n",
      "183/183 [==============================] - 0s 342us/step - loss: 0.3155 - acc: 0.8470 - val_loss: 0.3958 - val_acc: 0.8222\n",
      "Epoch 509/1000\n",
      "183/183 [==============================] - 0s 309us/step - loss: 0.3193 - acc: 0.8415 - val_loss: 0.3974 - val_acc: 0.8222\n",
      "Epoch 510/1000\n",
      "183/183 [==============================] - 0s 359us/step - loss: 0.3204 - acc: 0.8251 - val_loss: 0.4059 - val_acc: 0.8185\n",
      "Epoch 511/1000\n",
      "183/183 [==============================] - 0s 306us/step - loss: 0.3164 - acc: 0.8415 - val_loss: 0.3979 - val_acc: 0.8185\n",
      "Epoch 512/1000\n",
      "183/183 [==============================] - 0s 344us/step - loss: 0.3163 - acc: 0.8415 - val_loss: 0.3956 - val_acc: 0.8222\n",
      "Epoch 513/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.3189 - acc: 0.8415 - val_loss: 0.4046 - val_acc: 0.8111\n",
      "Epoch 514/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3171 - acc: 0.8525 - val_loss: 0.3966 - val_acc: 0.8222\n",
      "Epoch 515/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3167 - acc: 0.8470 - val_loss: 0.3974 - val_acc: 0.8296\n",
      "Epoch 516/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3214 - acc: 0.8361 - val_loss: 0.4023 - val_acc: 0.8185\n",
      "Epoch 517/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.3124 - acc: 0.8525 - val_loss: 0.3955 - val_acc: 0.8185\n",
      "Epoch 518/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3218 - acc: 0.8361 - val_loss: 0.3969 - val_acc: 0.8185\n",
      "Epoch 519/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3189 - acc: 0.8470 - val_loss: 0.4098 - val_acc: 0.8074\n",
      "Epoch 520/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.3194 - acc: 0.8470 - val_loss: 0.3949 - val_acc: 0.8296\n",
      "Epoch 521/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.3177 - acc: 0.8415 - val_loss: 0.3993 - val_acc: 0.8296\n",
      "Epoch 522/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.3119 - acc: 0.8579 - val_loss: 0.3973 - val_acc: 0.8222\n",
      "Epoch 523/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3139 - acc: 0.8525 - val_loss: 0.3983 - val_acc: 0.8185\n",
      "Epoch 524/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3124 - acc: 0.8470 - val_loss: 0.3981 - val_acc: 0.8222\n",
      "Epoch 525/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3115 - acc: 0.8415 - val_loss: 0.3942 - val_acc: 0.8259\n",
      "Epoch 526/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 288us/step - loss: 0.3112 - acc: 0.8470 - val_loss: 0.3954 - val_acc: 0.8296\n",
      "Epoch 527/1000\n",
      "183/183 [==============================] - 0s 284us/step - loss: 0.3140 - acc: 0.8470 - val_loss: 0.4012 - val_acc: 0.8296\n",
      "Epoch 528/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.3115 - acc: 0.8525 - val_loss: 0.3954 - val_acc: 0.8259\n",
      "Epoch 529/1000\n",
      "183/183 [==============================] - 0s 298us/step - loss: 0.3095 - acc: 0.8525 - val_loss: 0.3951 - val_acc: 0.8370\n",
      "Epoch 530/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.3095 - acc: 0.8579 - val_loss: 0.3946 - val_acc: 0.8259\n",
      "Epoch 531/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.3093 - acc: 0.8306 - val_loss: 0.3942 - val_acc: 0.8259\n",
      "Epoch 532/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.3063 - acc: 0.8525 - val_loss: 0.3989 - val_acc: 0.8148\n",
      "Epoch 533/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.3163 - acc: 0.8306 - val_loss: 0.4007 - val_acc: 0.8222\n",
      "Epoch 534/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3088 - acc: 0.8525 - val_loss: 0.3958 - val_acc: 0.8259\n",
      "Epoch 535/1000\n",
      "183/183 [==============================] - 0s 262us/step - loss: 0.3108 - acc: 0.8470 - val_loss: 0.3983 - val_acc: 0.8148\n",
      "Epoch 536/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.3063 - acc: 0.8579 - val_loss: 0.3922 - val_acc: 0.8296\n",
      "Epoch 537/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3107 - acc: 0.8579 - val_loss: 0.3930 - val_acc: 0.8407\n",
      "Epoch 538/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.3074 - acc: 0.8634 - val_loss: 0.3927 - val_acc: 0.8370\n",
      "Epoch 539/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.3052 - acc: 0.8634 - val_loss: 0.3972 - val_acc: 0.8222\n",
      "Epoch 540/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.3078 - acc: 0.8470 - val_loss: 0.3968 - val_acc: 0.8333\n",
      "Epoch 541/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3139 - acc: 0.8470 - val_loss: 0.3931 - val_acc: 0.8296\n",
      "Epoch 542/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.3152 - acc: 0.8415 - val_loss: 0.4059 - val_acc: 0.8111\n",
      "Epoch 543/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.3061 - acc: 0.8579 - val_loss: 0.3929 - val_acc: 0.8333\n",
      "Epoch 544/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3101 - acc: 0.8525 - val_loss: 0.3908 - val_acc: 0.8222\n",
      "Epoch 545/1000\n",
      "183/183 [==============================] - 0s 259us/step - loss: 0.3042 - acc: 0.8525 - val_loss: 0.4002 - val_acc: 0.8222\n",
      "Epoch 546/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.3082 - acc: 0.8470 - val_loss: 0.3946 - val_acc: 0.8333\n",
      "Epoch 547/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.3038 - acc: 0.8470 - val_loss: 0.3923 - val_acc: 0.8259\n",
      "Epoch 548/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.3111 - acc: 0.8525 - val_loss: 0.3937 - val_acc: 0.8222\n",
      "Epoch 549/1000\n",
      "183/183 [==============================] - 0s 280us/step - loss: 0.3021 - acc: 0.8579 - val_loss: 0.3901 - val_acc: 0.8259\n",
      "Epoch 550/1000\n",
      "183/183 [==============================] - 0s 230us/step - loss: 0.3033 - acc: 0.8525 - val_loss: 0.3950 - val_acc: 0.8259\n",
      "Epoch 551/1000\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3158 - acc: 0.875 - 0s 282us/step - loss: 0.3060 - acc: 0.8525 - val_loss: 0.3984 - val_acc: 0.8222\n",
      "Epoch 552/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2994 - acc: 0.8525 - val_loss: 0.3919 - val_acc: 0.8222\n",
      "Epoch 553/1000\n",
      "183/183 [==============================] - 0s 295us/step - loss: 0.3121 - acc: 0.8306 - val_loss: 0.3905 - val_acc: 0.8259\n",
      "Epoch 554/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.3046 - acc: 0.8470 - val_loss: 0.3987 - val_acc: 0.8259\n",
      "Epoch 555/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.3064 - acc: 0.8470 - val_loss: 0.3940 - val_acc: 0.8333\n",
      "Epoch 556/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.2983 - acc: 0.8689 - val_loss: 0.3932 - val_acc: 0.8259\n",
      "Epoch 557/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.3020 - acc: 0.8579 - val_loss: 0.3948 - val_acc: 0.8222\n",
      "Epoch 558/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.2957 - acc: 0.8634 - val_loss: 0.3899 - val_acc: 0.8296\n",
      "Epoch 559/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.3013 - acc: 0.8579 - val_loss: 0.3900 - val_acc: 0.8259\n",
      "Epoch 560/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2968 - acc: 0.8579 - val_loss: 0.3919 - val_acc: 0.8259\n",
      "Epoch 561/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.2960 - acc: 0.8579 - val_loss: 0.3977 - val_acc: 0.8259\n",
      "Epoch 562/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2975 - acc: 0.8579 - val_loss: 0.3910 - val_acc: 0.8296\n",
      "Epoch 563/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.2956 - acc: 0.8689 - val_loss: 0.3915 - val_acc: 0.8259\n",
      "Epoch 564/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.2970 - acc: 0.8634 - val_loss: 0.3921 - val_acc: 0.8333\n",
      "Epoch 565/1000\n",
      "183/183 [==============================] - 0s 291us/step - loss: 0.2980 - acc: 0.8579 - val_loss: 0.3899 - val_acc: 0.8296\n",
      "Epoch 566/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2956 - acc: 0.8634 - val_loss: 0.3933 - val_acc: 0.8259\n",
      "Epoch 567/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2992 - acc: 0.8525 - val_loss: 0.3900 - val_acc: 0.8259\n",
      "Epoch 568/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.2972 - acc: 0.8579 - val_loss: 0.3914 - val_acc: 0.8185\n",
      "Epoch 569/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2931 - acc: 0.8634 - val_loss: 0.3892 - val_acc: 0.8259\n",
      "Epoch 570/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2946 - acc: 0.8634 - val_loss: 0.3885 - val_acc: 0.8333\n",
      "Epoch 571/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2916 - acc: 0.8634 - val_loss: 0.3893 - val_acc: 0.8259\n",
      "Epoch 572/1000\n",
      "183/183 [==============================] - 0s 235us/step - loss: 0.2937 - acc: 0.8634 - val_loss: 0.3945 - val_acc: 0.8259\n",
      "Epoch 573/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2925 - acc: 0.8743 - val_loss: 0.3865 - val_acc: 0.8296\n",
      "Epoch 574/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2918 - acc: 0.8634 - val_loss: 0.3883 - val_acc: 0.8222\n",
      "Epoch 575/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2922 - acc: 0.8579 - val_loss: 0.3918 - val_acc: 0.8333\n",
      "Epoch 576/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2908 - acc: 0.8689 - val_loss: 0.3914 - val_acc: 0.8296\n",
      "Epoch 577/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.2904 - acc: 0.8579 - val_loss: 0.3882 - val_acc: 0.8296\n",
      "Epoch 578/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.2896 - acc: 0.8634 - val_loss: 0.3884 - val_acc: 0.8185\n",
      "Epoch 579/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2911 - acc: 0.8579 - val_loss: 0.3884 - val_acc: 0.8296\n",
      "Epoch 580/1000\n",
      "183/183 [==============================] - 0s 305us/step - loss: 0.2855 - acc: 0.8689 - val_loss: 0.3890 - val_acc: 0.8370\n",
      "Epoch 581/1000\n",
      "183/183 [==============================] - 0s 253us/step - loss: 0.2892 - acc: 0.8634 - val_loss: 0.3917 - val_acc: 0.8222\n",
      "Epoch 582/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2904 - acc: 0.8634 - val_loss: 0.3882 - val_acc: 0.8296\n",
      "Epoch 583/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2905 - acc: 0.8525 - val_loss: 0.3845 - val_acc: 0.8444\n",
      "Epoch 584/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2891 - acc: 0.8634 - val_loss: 0.3908 - val_acc: 0.8259\n",
      "Epoch 585/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 237us/step - loss: 0.2874 - acc: 0.8634 - val_loss: 0.3913 - val_acc: 0.8259\n",
      "Epoch 586/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.2845 - acc: 0.8634 - val_loss: 0.3857 - val_acc: 0.8333\n",
      "Epoch 587/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.2873 - acc: 0.8579 - val_loss: 0.3856 - val_acc: 0.8333\n",
      "Epoch 588/1000\n",
      "183/183 [==============================] - 0s 273us/step - loss: 0.2878 - acc: 0.8579 - val_loss: 0.3853 - val_acc: 0.8259\n",
      "Epoch 589/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2866 - acc: 0.8579 - val_loss: 0.3909 - val_acc: 0.8259\n",
      "Epoch 590/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2841 - acc: 0.8579 - val_loss: 0.3834 - val_acc: 0.8296\n",
      "Epoch 591/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2830 - acc: 0.8689 - val_loss: 0.3852 - val_acc: 0.8222\n",
      "Epoch 592/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2827 - acc: 0.8579 - val_loss: 0.3859 - val_acc: 0.8222\n",
      "Epoch 593/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.2817 - acc: 0.8634 - val_loss: 0.3849 - val_acc: 0.8296\n",
      "Epoch 594/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.2820 - acc: 0.8579 - val_loss: 0.3854 - val_acc: 0.8259\n",
      "Epoch 595/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.2899 - acc: 0.8579 - val_loss: 0.3842 - val_acc: 0.8259\n",
      "Epoch 596/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.2825 - acc: 0.8743 - val_loss: 0.3915 - val_acc: 0.8296\n",
      "Epoch 597/1000\n",
      "183/183 [==============================] - 0s 254us/step - loss: 0.2806 - acc: 0.8634 - val_loss: 0.3824 - val_acc: 0.8296\n",
      "Epoch 598/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2802 - acc: 0.8743 - val_loss: 0.3800 - val_acc: 0.8407\n",
      "Epoch 599/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2809 - acc: 0.8743 - val_loss: 0.3824 - val_acc: 0.8407\n",
      "Epoch 600/1000\n",
      "183/183 [==============================] - 0s 243us/step - loss: 0.2790 - acc: 0.8525 - val_loss: 0.3842 - val_acc: 0.8185\n",
      "Epoch 601/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2789 - acc: 0.8470 - val_loss: 0.3854 - val_acc: 0.8185\n",
      "Epoch 602/1000\n",
      "183/183 [==============================] - 0s 280us/step - loss: 0.2787 - acc: 0.8798 - val_loss: 0.3833 - val_acc: 0.8407\n",
      "Epoch 603/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.2860 - acc: 0.8634 - val_loss: 0.3853 - val_acc: 0.8185\n",
      "Epoch 604/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.2721 - acc: 0.8743 - val_loss: 0.3831 - val_acc: 0.8481\n",
      "Epoch 605/1000\n",
      "183/183 [==============================] - 0s 245us/step - loss: 0.2805 - acc: 0.8798 - val_loss: 0.3814 - val_acc: 0.8333\n",
      "Epoch 606/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2757 - acc: 0.8634 - val_loss: 0.3850 - val_acc: 0.8222\n",
      "Epoch 607/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.2774 - acc: 0.8579 - val_loss: 0.3814 - val_acc: 0.8407\n",
      "Epoch 608/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.2740 - acc: 0.8798 - val_loss: 0.3837 - val_acc: 0.8444\n",
      "Epoch 609/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2758 - acc: 0.8852 - val_loss: 0.3819 - val_acc: 0.8296\n",
      "Epoch 610/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.2736 - acc: 0.8798 - val_loss: 0.3820 - val_acc: 0.8222\n",
      "Epoch 611/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2744 - acc: 0.8470 - val_loss: 0.3827 - val_acc: 0.8222\n",
      "Epoch 612/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2729 - acc: 0.8689 - val_loss: 0.3799 - val_acc: 0.8333\n",
      "Epoch 613/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2718 - acc: 0.8798 - val_loss: 0.3802 - val_acc: 0.8370\n",
      "Epoch 614/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.2736 - acc: 0.8689 - val_loss: 0.3788 - val_acc: 0.8407\n",
      "Epoch 615/1000\n",
      "183/183 [==============================] - 0s 270us/step - loss: 0.2709 - acc: 0.8634 - val_loss: 0.3781 - val_acc: 0.8333\n",
      "Epoch 616/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2700 - acc: 0.8852 - val_loss: 0.3828 - val_acc: 0.8185\n",
      "Epoch 617/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.2710 - acc: 0.8743 - val_loss: 0.3799 - val_acc: 0.8370\n",
      "Epoch 618/1000\n",
      "183/183 [==============================] - 0s 250us/step - loss: 0.2703 - acc: 0.8689 - val_loss: 0.3827 - val_acc: 0.8296\n",
      "Epoch 619/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.2764 - acc: 0.8798 - val_loss: 0.3782 - val_acc: 0.8481\n",
      "Epoch 620/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.2691 - acc: 0.8689 - val_loss: 0.3855 - val_acc: 0.8259\n",
      "Epoch 621/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2702 - acc: 0.8470 - val_loss: 0.3803 - val_acc: 0.8259\n",
      "Epoch 622/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2713 - acc: 0.8798 - val_loss: 0.3780 - val_acc: 0.8370\n",
      "Epoch 623/1000\n",
      "183/183 [==============================] - 0s 267us/step - loss: 0.2653 - acc: 0.8743 - val_loss: 0.3775 - val_acc: 0.8296\n",
      "Epoch 624/1000\n",
      "183/183 [==============================] - 0s 253us/step - loss: 0.2750 - acc: 0.8852 - val_loss: 0.3771 - val_acc: 0.8259\n",
      "Epoch 625/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2664 - acc: 0.8689 - val_loss: 0.3876 - val_acc: 0.8296\n",
      "Epoch 626/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2652 - acc: 0.8743 - val_loss: 0.3775 - val_acc: 0.8333\n",
      "Epoch 627/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2666 - acc: 0.8798 - val_loss: 0.3749 - val_acc: 0.8593\n",
      "Epoch 628/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.2698 - acc: 0.8579 - val_loss: 0.3790 - val_acc: 0.8222\n",
      "Epoch 629/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.2635 - acc: 0.8689 - val_loss: 0.3769 - val_acc: 0.8333\n",
      "Epoch 630/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2634 - acc: 0.8852 - val_loss: 0.3778 - val_acc: 0.8296\n",
      "Epoch 631/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2650 - acc: 0.8579 - val_loss: 0.3777 - val_acc: 0.8259\n",
      "Epoch 632/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2633 - acc: 0.8798 - val_loss: 0.3770 - val_acc: 0.8444\n",
      "Epoch 633/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.2603 - acc: 0.8907 - val_loss: 0.3811 - val_acc: 0.8259\n",
      "Epoch 634/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2646 - acc: 0.8798 - val_loss: 0.3752 - val_acc: 0.8333\n",
      "Epoch 635/1000\n",
      "183/183 [==============================] - 0s 237us/step - loss: 0.2618 - acc: 0.8852 - val_loss: 0.3780 - val_acc: 0.8185\n",
      "Epoch 636/1000\n",
      "183/183 [==============================] - 0s 272us/step - loss: 0.2605 - acc: 0.8689 - val_loss: 0.3765 - val_acc: 0.8407\n",
      "Epoch 637/1000\n",
      "183/183 [==============================] - 0s 261us/step - loss: 0.2650 - acc: 0.8852 - val_loss: 0.3758 - val_acc: 0.8296\n",
      "Epoch 638/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.2632 - acc: 0.8962 - val_loss: 0.3796 - val_acc: 0.8370\n",
      "Epoch 639/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2605 - acc: 0.9016 - val_loss: 0.3725 - val_acc: 0.8407\n",
      "Epoch 640/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2625 - acc: 0.8689 - val_loss: 0.3793 - val_acc: 0.8222\n",
      "Epoch 641/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2588 - acc: 0.8798 - val_loss: 0.3745 - val_acc: 0.8444\n",
      "Epoch 642/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.2564 - acc: 0.8852 - val_loss: 0.3751 - val_acc: 0.8370\n",
      "Epoch 643/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.2578 - acc: 0.8798 - val_loss: 0.3745 - val_acc: 0.8444\n",
      "Epoch 644/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 292us/step - loss: 0.2655 - acc: 0.8634 - val_loss: 0.3781 - val_acc: 0.8407\n",
      "Epoch 645/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.2653 - acc: 0.8798 - val_loss: 0.3749 - val_acc: 0.8630\n",
      "Epoch 646/1000\n",
      "183/183 [==============================] - 0s 294us/step - loss: 0.2566 - acc: 0.8798 - val_loss: 0.3748 - val_acc: 0.8407\n",
      "Epoch 647/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2603 - acc: 0.8852 - val_loss: 0.3824 - val_acc: 0.8370\n",
      "Epoch 648/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2562 - acc: 0.8962 - val_loss: 0.3735 - val_acc: 0.8407\n",
      "Epoch 649/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2569 - acc: 0.8852 - val_loss: 0.3725 - val_acc: 0.8370\n",
      "Epoch 650/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2551 - acc: 0.8798 - val_loss: 0.3707 - val_acc: 0.8370\n",
      "Epoch 651/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.2532 - acc: 0.8798 - val_loss: 0.3719 - val_acc: 0.8407\n",
      "Epoch 652/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2542 - acc: 0.8962 - val_loss: 0.3711 - val_acc: 0.8519\n",
      "Epoch 653/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2511 - acc: 0.8962 - val_loss: 0.3692 - val_acc: 0.8519\n",
      "Epoch 654/1000\n",
      "183/183 [==============================] - 0s 275us/step - loss: 0.2528 - acc: 0.8907 - val_loss: 0.3722 - val_acc: 0.8407\n",
      "Epoch 655/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.2522 - acc: 0.8798 - val_loss: 0.3753 - val_acc: 0.8407\n",
      "Epoch 656/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.2531 - acc: 0.8798 - val_loss: 0.3715 - val_acc: 0.8407\n",
      "Epoch 657/1000\n",
      "183/183 [==============================] - 0s 230us/step - loss: 0.2531 - acc: 0.9071 - val_loss: 0.3761 - val_acc: 0.8444\n",
      "Epoch 658/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.2550 - acc: 0.8907 - val_loss: 0.3697 - val_acc: 0.8556\n",
      "Epoch 659/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.2495 - acc: 0.8907 - val_loss: 0.3700 - val_acc: 0.8370\n",
      "Epoch 660/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.2562 - acc: 0.8689 - val_loss: 0.3819 - val_acc: 0.8296\n",
      "Epoch 661/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2502 - acc: 0.8743 - val_loss: 0.3730 - val_acc: 0.8481\n",
      "Epoch 662/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2492 - acc: 0.8852 - val_loss: 0.3702 - val_acc: 0.8481\n",
      "Epoch 663/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2529 - acc: 0.8689 - val_loss: 0.3723 - val_acc: 0.8370\n",
      "Epoch 664/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2478 - acc: 0.9016 - val_loss: 0.3661 - val_acc: 0.8444\n",
      "Epoch 665/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.2462 - acc: 0.8907 - val_loss: 0.3683 - val_acc: 0.8444\n",
      "Epoch 666/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2453 - acc: 0.8907 - val_loss: 0.3719 - val_acc: 0.8370\n",
      "Epoch 667/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2453 - acc: 0.9016 - val_loss: 0.3751 - val_acc: 0.8370\n",
      "Epoch 668/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.2477 - acc: 0.8962 - val_loss: 0.3670 - val_acc: 0.8481\n",
      "Epoch 669/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.2441 - acc: 0.8907 - val_loss: 0.3681 - val_acc: 0.8444\n",
      "Epoch 670/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.2451 - acc: 0.8798 - val_loss: 0.3714 - val_acc: 0.8259\n",
      "Epoch 671/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.2434 - acc: 0.8798 - val_loss: 0.3681 - val_acc: 0.8444\n",
      "Epoch 672/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2421 - acc: 0.8907 - val_loss: 0.3721 - val_acc: 0.8519\n",
      "Epoch 673/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2445 - acc: 0.9016 - val_loss: 0.3715 - val_acc: 0.8519\n",
      "Epoch 674/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2402 - acc: 0.8962 - val_loss: 0.3665 - val_acc: 0.8481\n",
      "Epoch 675/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.2460 - acc: 0.9016 - val_loss: 0.3665 - val_acc: 0.8444\n",
      "Epoch 676/1000\n",
      "183/183 [==============================] - 0s 235us/step - loss: 0.2470 - acc: 0.8798 - val_loss: 0.3707 - val_acc: 0.8519\n",
      "Epoch 677/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.2411 - acc: 0.8962 - val_loss: 0.3693 - val_acc: 0.8481\n",
      "Epoch 678/1000\n",
      "183/183 [==============================] - 0s 222us/step - loss: 0.2490 - acc: 0.8907 - val_loss: 0.3698 - val_acc: 0.8519\n",
      "Epoch 679/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2443 - acc: 0.8525 - val_loss: 0.3806 - val_acc: 0.8185\n",
      "Epoch 680/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2394 - acc: 0.8907 - val_loss: 0.3662 - val_acc: 0.8481\n",
      "Epoch 681/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.2432 - acc: 0.8907 - val_loss: 0.3658 - val_acc: 0.8556\n",
      "Epoch 682/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.2472 - acc: 0.9016 - val_loss: 0.3651 - val_acc: 0.8593\n",
      "Epoch 683/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2395 - acc: 0.8962 - val_loss: 0.3748 - val_acc: 0.8370\n",
      "Epoch 684/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2386 - acc: 0.8907 - val_loss: 0.3652 - val_acc: 0.8519\n",
      "Epoch 685/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.2374 - acc: 0.8962 - val_loss: 0.3682 - val_acc: 0.8630\n",
      "Epoch 686/1000\n",
      "183/183 [==============================] - 0s 225us/step - loss: 0.2363 - acc: 0.9016 - val_loss: 0.3713 - val_acc: 0.8481\n",
      "Epoch 687/1000\n",
      "183/183 [==============================] - 0s 275us/step - loss: 0.2364 - acc: 0.8907 - val_loss: 0.3683 - val_acc: 0.8444\n",
      "Epoch 688/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2358 - acc: 0.9071 - val_loss: 0.3647 - val_acc: 0.8556\n",
      "Epoch 689/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.2348 - acc: 0.9235 - val_loss: 0.3698 - val_acc: 0.8407\n",
      "Epoch 690/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2352 - acc: 0.8962 - val_loss: 0.3664 - val_acc: 0.8556\n",
      "Epoch 691/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.2354 - acc: 0.8962 - val_loss: 0.3654 - val_acc: 0.8704\n",
      "Epoch 692/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2320 - acc: 0.9126 - val_loss: 0.3670 - val_acc: 0.8444\n",
      "Epoch 693/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2345 - acc: 0.9016 - val_loss: 0.3631 - val_acc: 0.8556\n",
      "Epoch 694/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.2329 - acc: 0.9016 - val_loss: 0.3692 - val_acc: 0.8444\n",
      "Epoch 695/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.2351 - acc: 0.9071 - val_loss: 0.3596 - val_acc: 0.8593\n",
      "Epoch 696/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2350 - acc: 0.9235 - val_loss: 0.3637 - val_acc: 0.8593\n",
      "Epoch 697/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2327 - acc: 0.8907 - val_loss: 0.3690 - val_acc: 0.8407\n",
      "Epoch 698/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2308 - acc: 0.9016 - val_loss: 0.3691 - val_acc: 0.8556\n",
      "Epoch 699/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2306 - acc: 0.9180 - val_loss: 0.3656 - val_acc: 0.8556\n",
      "Epoch 700/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.2313 - acc: 0.9016 - val_loss: 0.3624 - val_acc: 0.8630\n",
      "Epoch 701/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.2272 - acc: 0.9290 - val_loss: 0.3667 - val_acc: 0.8481\n",
      "Epoch 702/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.2342 - acc: 0.9016 - val_loss: 0.3646 - val_acc: 0.8630\n",
      "Epoch 703/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 225us/step - loss: 0.2294 - acc: 0.9126 - val_loss: 0.3720 - val_acc: 0.8630\n",
      "Epoch 704/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.2373 - acc: 0.8907 - val_loss: 0.3730 - val_acc: 0.8481\n",
      "Epoch 705/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2386 - acc: 0.8907 - val_loss: 0.3704 - val_acc: 0.8407\n",
      "Epoch 706/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.2421 - acc: 0.8907 - val_loss: 0.3674 - val_acc: 0.8630\n",
      "Epoch 707/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.2312 - acc: 0.9290 - val_loss: 0.3647 - val_acc: 0.8593\n",
      "Epoch 708/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2311 - acc: 0.8962 - val_loss: 0.3697 - val_acc: 0.8407\n",
      "Epoch 709/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.2316 - acc: 0.8907 - val_loss: 0.3687 - val_acc: 0.8370\n",
      "Epoch 710/1000\n",
      "183/183 [==============================] - 0s 284us/step - loss: 0.2232 - acc: 0.9071 - val_loss: 0.3635 - val_acc: 0.8778\n",
      "Epoch 711/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.2277 - acc: 0.9235 - val_loss: 0.3618 - val_acc: 0.8704\n",
      "Epoch 712/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2300 - acc: 0.9016 - val_loss: 0.3615 - val_acc: 0.8481\n",
      "Epoch 713/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2272 - acc: 0.8907 - val_loss: 0.3614 - val_acc: 0.8556\n",
      "Epoch 714/1000\n",
      "183/183 [==============================] - 0s 262us/step - loss: 0.2254 - acc: 0.9180 - val_loss: 0.3654 - val_acc: 0.8741\n",
      "Epoch 715/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.2239 - acc: 0.9235 - val_loss: 0.3640 - val_acc: 0.8481\n",
      "Epoch 716/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2211 - acc: 0.9126 - val_loss: 0.3617 - val_acc: 0.8556\n",
      "Epoch 717/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2210 - acc: 0.9126 - val_loss: 0.3614 - val_acc: 0.8667\n",
      "Epoch 718/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2192 - acc: 0.9180 - val_loss: 0.3615 - val_acc: 0.8556\n",
      "Epoch 719/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2289 - acc: 0.9071 - val_loss: 0.3654 - val_acc: 0.8519\n",
      "Epoch 720/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.2197 - acc: 0.9180 - val_loss: 0.3677 - val_acc: 0.8667\n",
      "Epoch 721/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2218 - acc: 0.9180 - val_loss: 0.3608 - val_acc: 0.8630\n",
      "Epoch 722/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.2210 - acc: 0.9071 - val_loss: 0.3633 - val_acc: 0.8519\n",
      "Epoch 723/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2218 - acc: 0.9016 - val_loss: 0.3617 - val_acc: 0.8519\n",
      "Epoch 724/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2208 - acc: 0.8962 - val_loss: 0.3665 - val_acc: 0.8519\n",
      "Epoch 725/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2183 - acc: 0.9180 - val_loss: 0.3659 - val_acc: 0.8556\n",
      "Epoch 726/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.2317 - acc: 0.8907 - val_loss: 0.3681 - val_acc: 0.8519\n",
      "Epoch 727/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.2261 - acc: 0.9016 - val_loss: 0.3672 - val_acc: 0.8630\n",
      "Epoch 728/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2283 - acc: 0.9071 - val_loss: 0.3577 - val_acc: 0.8704\n",
      "Epoch 729/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2202 - acc: 0.9071 - val_loss: 0.3581 - val_acc: 0.8556\n",
      "Epoch 730/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2172 - acc: 0.9016 - val_loss: 0.3590 - val_acc: 0.8667\n",
      "Epoch 731/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.2275 - acc: 0.9071 - val_loss: 0.3612 - val_acc: 0.8704\n",
      "Epoch 732/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2167 - acc: 0.9180 - val_loss: 0.3703 - val_acc: 0.8407\n",
      "Epoch 733/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2236 - acc: 0.8852 - val_loss: 0.3612 - val_acc: 0.8704\n",
      "Epoch 734/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.2219 - acc: 0.9071 - val_loss: 0.3603 - val_acc: 0.8667\n",
      "Epoch 735/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.2172 - acc: 0.9180 - val_loss: 0.3628 - val_acc: 0.8630\n",
      "Epoch 736/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.2150 - acc: 0.9235 - val_loss: 0.3648 - val_acc: 0.8519\n",
      "Epoch 737/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.2150 - acc: 0.9180 - val_loss: 0.3669 - val_acc: 0.8519\n",
      "Epoch 738/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.2126 - acc: 0.9180 - val_loss: 0.3568 - val_acc: 0.8667\n",
      "Epoch 739/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.2160 - acc: 0.9180 - val_loss: 0.3589 - val_acc: 0.8778\n",
      "Epoch 740/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.2109 - acc: 0.9180 - val_loss: 0.3656 - val_acc: 0.8519\n",
      "Epoch 741/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.2156 - acc: 0.9126 - val_loss: 0.3635 - val_acc: 0.8481\n",
      "Epoch 742/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2185 - acc: 0.9126 - val_loss: 0.3609 - val_acc: 0.8667\n",
      "Epoch 743/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.2123 - acc: 0.9180 - val_loss: 0.3644 - val_acc: 0.8630\n",
      "Epoch 744/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.2117 - acc: 0.9126 - val_loss: 0.3615 - val_acc: 0.8593\n",
      "Epoch 745/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2103 - acc: 0.9016 - val_loss: 0.3562 - val_acc: 0.8630\n",
      "Epoch 746/1000\n",
      "183/183 [==============================] - 0s 235us/step - loss: 0.2072 - acc: 0.9180 - val_loss: 0.3588 - val_acc: 0.8741\n",
      "Epoch 747/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.2090 - acc: 0.9235 - val_loss: 0.3628 - val_acc: 0.8667\n",
      "Epoch 748/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.2107 - acc: 0.9235 - val_loss: 0.3614 - val_acc: 0.8593\n",
      "Epoch 749/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2103 - acc: 0.9180 - val_loss: 0.3597 - val_acc: 0.8630\n",
      "Epoch 750/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.2073 - acc: 0.9180 - val_loss: 0.3569 - val_acc: 0.8630\n",
      "Epoch 751/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.2098 - acc: 0.9180 - val_loss: 0.3570 - val_acc: 0.8556\n",
      "Epoch 752/1000\n",
      "183/183 [==============================] - 0s 273us/step - loss: 0.2086 - acc: 0.9126 - val_loss: 0.3613 - val_acc: 0.8593\n",
      "Epoch 753/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2082 - acc: 0.9126 - val_loss: 0.3636 - val_acc: 0.8667\n",
      "Epoch 754/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.2109 - acc: 0.9180 - val_loss: 0.3635 - val_acc: 0.8519\n",
      "Epoch 755/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.2051 - acc: 0.9235 - val_loss: 0.3582 - val_acc: 0.8630\n",
      "Epoch 756/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.2102 - acc: 0.9126 - val_loss: 0.3623 - val_acc: 0.8556\n",
      "Epoch 757/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.2106 - acc: 0.9126 - val_loss: 0.3643 - val_acc: 0.8667\n",
      "Epoch 758/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.2045 - acc: 0.9290 - val_loss: 0.3626 - val_acc: 0.8630\n",
      "Epoch 759/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2051 - acc: 0.9180 - val_loss: 0.3616 - val_acc: 0.8593\n",
      "Epoch 760/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2053 - acc: 0.9180 - val_loss: 0.3573 - val_acc: 0.8630\n",
      "Epoch 761/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2114 - acc: 0.9071 - val_loss: 0.3679 - val_acc: 0.8593\n",
      "Epoch 762/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 266us/step - loss: 0.2085 - acc: 0.9071 - val_loss: 0.3636 - val_acc: 0.8630\n",
      "Epoch 763/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.2190 - acc: 0.9180 - val_loss: 0.3602 - val_acc: 0.8667\n",
      "Epoch 764/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2126 - acc: 0.9180 - val_loss: 0.3673 - val_acc: 0.8593\n",
      "Epoch 765/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.2056 - acc: 0.9126 - val_loss: 0.3603 - val_acc: 0.8704\n",
      "Epoch 766/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.2046 - acc: 0.9126 - val_loss: 0.3584 - val_acc: 0.8667\n",
      "Epoch 767/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2119 - acc: 0.9180 - val_loss: 0.3679 - val_acc: 0.8630\n",
      "Epoch 768/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.2038 - acc: 0.9126 - val_loss: 0.3621 - val_acc: 0.8556\n",
      "Epoch 769/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.2038 - acc: 0.9180 - val_loss: 0.3562 - val_acc: 0.8667\n",
      "Epoch 770/1000\n",
      "183/183 [==============================] - 0s 337us/step - loss: 0.2003 - acc: 0.9180 - val_loss: 0.3550 - val_acc: 0.8704\n",
      "Epoch 771/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.2016 - acc: 0.9235 - val_loss: 0.3568 - val_acc: 0.8778\n",
      "Epoch 772/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1973 - acc: 0.9290 - val_loss: 0.3666 - val_acc: 0.8556\n",
      "Epoch 773/1000\n",
      "183/183 [==============================] - 0s 297us/step - loss: 0.2027 - acc: 0.9180 - val_loss: 0.3611 - val_acc: 0.8556\n",
      "Epoch 774/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1934 - acc: 0.9235 - val_loss: 0.3650 - val_acc: 0.8593\n",
      "Epoch 775/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.2021 - acc: 0.9180 - val_loss: 0.3563 - val_acc: 0.8704\n",
      "Epoch 776/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.2016 - acc: 0.9180 - val_loss: 0.3568 - val_acc: 0.8667\n",
      "Epoch 777/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.1954 - acc: 0.9290 - val_loss: 0.3579 - val_acc: 0.8630\n",
      "Epoch 778/1000\n",
      "183/183 [==============================] - 0s 310us/step - loss: 0.1976 - acc: 0.9126 - val_loss: 0.3595 - val_acc: 0.8667\n",
      "Epoch 779/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1951 - acc: 0.9180 - val_loss: 0.3583 - val_acc: 0.8630\n",
      "Epoch 780/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1979 - acc: 0.9180 - val_loss: 0.3578 - val_acc: 0.8704\n",
      "Epoch 781/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1980 - acc: 0.9235 - val_loss: 0.3623 - val_acc: 0.8481\n",
      "Epoch 782/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1957 - acc: 0.9126 - val_loss: 0.3592 - val_acc: 0.8630\n",
      "Epoch 783/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.1986 - acc: 0.9235 - val_loss: 0.3591 - val_acc: 0.8704\n",
      "Epoch 784/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1960 - acc: 0.9235 - val_loss: 0.3586 - val_acc: 0.8630\n",
      "Epoch 785/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.2006 - acc: 0.9180 - val_loss: 0.3581 - val_acc: 0.8667\n",
      "Epoch 786/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1985 - acc: 0.9071 - val_loss: 0.3637 - val_acc: 0.8630\n",
      "Epoch 787/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1920 - acc: 0.9290 - val_loss: 0.3623 - val_acc: 0.8593\n",
      "Epoch 788/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.1954 - acc: 0.9290 - val_loss: 0.3607 - val_acc: 0.8519\n",
      "Epoch 789/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1979 - acc: 0.9071 - val_loss: 0.3608 - val_acc: 0.8481\n",
      "Epoch 790/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1926 - acc: 0.9180 - val_loss: 0.3646 - val_acc: 0.8593\n",
      "Epoch 791/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1991 - acc: 0.9126 - val_loss: 0.3625 - val_acc: 0.8667\n",
      "Epoch 792/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1926 - acc: 0.9290 - val_loss: 0.3599 - val_acc: 0.8630\n",
      "Epoch 793/1000\n",
      "183/183 [==============================] - 0s 261us/step - loss: 0.1936 - acc: 0.9290 - val_loss: 0.3576 - val_acc: 0.8593\n",
      "Epoch 794/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.1927 - acc: 0.9290 - val_loss: 0.3616 - val_acc: 0.8593\n",
      "Epoch 795/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.1891 - acc: 0.9180 - val_loss: 0.3633 - val_acc: 0.8556\n",
      "Epoch 796/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.1944 - acc: 0.9126 - val_loss: 0.3579 - val_acc: 0.8704\n",
      "Epoch 797/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1947 - acc: 0.9344 - val_loss: 0.3583 - val_acc: 0.8667\n",
      "Epoch 798/1000\n",
      "183/183 [==============================] - 0s 246us/step - loss: 0.1885 - acc: 0.9235 - val_loss: 0.3571 - val_acc: 0.8667\n",
      "Epoch 799/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.1932 - acc: 0.9180 - val_loss: 0.3629 - val_acc: 0.8667\n",
      "Epoch 800/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.1900 - acc: 0.9235 - val_loss: 0.3588 - val_acc: 0.8630\n",
      "Epoch 801/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1902 - acc: 0.9344 - val_loss: 0.3559 - val_acc: 0.8593\n",
      "Epoch 802/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.1950 - acc: 0.9290 - val_loss: 0.3652 - val_acc: 0.8593\n",
      "Epoch 803/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1857 - acc: 0.9126 - val_loss: 0.3628 - val_acc: 0.8667\n",
      "Epoch 804/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1900 - acc: 0.9344 - val_loss: 0.3580 - val_acc: 0.8630\n",
      "Epoch 805/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.1872 - acc: 0.9290 - val_loss: 0.3612 - val_acc: 0.8593\n",
      "Epoch 806/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1900 - acc: 0.9235 - val_loss: 0.3592 - val_acc: 0.8704\n",
      "Epoch 807/1000\n",
      "183/183 [==============================] - 0s 307us/step - loss: 0.1877 - acc: 0.9290 - val_loss: 0.3638 - val_acc: 0.8704\n",
      "Epoch 808/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.1871 - acc: 0.9290 - val_loss: 0.3606 - val_acc: 0.8667\n",
      "Epoch 809/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1825 - acc: 0.9290 - val_loss: 0.3580 - val_acc: 0.8667\n",
      "Epoch 810/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1844 - acc: 0.9344 - val_loss: 0.3574 - val_acc: 0.8741\n",
      "Epoch 811/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.1833 - acc: 0.9344 - val_loss: 0.3606 - val_acc: 0.8667\n",
      "Epoch 812/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.1834 - acc: 0.9344 - val_loss: 0.3605 - val_acc: 0.8630\n",
      "Epoch 813/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1822 - acc: 0.9399 - val_loss: 0.3573 - val_acc: 0.8778\n",
      "Epoch 814/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.1868 - acc: 0.9344 - val_loss: 0.3615 - val_acc: 0.8630\n",
      "Epoch 815/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1876 - acc: 0.9290 - val_loss: 0.3598 - val_acc: 0.8704\n",
      "Epoch 816/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.1820 - acc: 0.9290 - val_loss: 0.3584 - val_acc: 0.8778\n",
      "Epoch 817/1000\n",
      "183/183 [==============================] - 0s 318us/step - loss: 0.1828 - acc: 0.9344 - val_loss: 0.3601 - val_acc: 0.8593\n",
      "Epoch 818/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1839 - acc: 0.9180 - val_loss: 0.3629 - val_acc: 0.8704\n",
      "Epoch 819/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.1807 - acc: 0.9290 - val_loss: 0.3638 - val_acc: 0.8704\n",
      "Epoch 820/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1899 - acc: 0.9290 - val_loss: 0.3634 - val_acc: 0.8667\n",
      "Epoch 821/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 230us/step - loss: 0.1915 - acc: 0.9180 - val_loss: 0.3672 - val_acc: 0.8556\n",
      "Epoch 822/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.1806 - acc: 0.9290 - val_loss: 0.3552 - val_acc: 0.8704\n",
      "Epoch 823/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1804 - acc: 0.9399 - val_loss: 0.3580 - val_acc: 0.8741\n",
      "Epoch 824/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.1781 - acc: 0.9344 - val_loss: 0.3651 - val_acc: 0.8593\n",
      "Epoch 825/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.1841 - acc: 0.9344 - val_loss: 0.3654 - val_acc: 0.8519\n",
      "Epoch 826/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.1820 - acc: 0.9071 - val_loss: 0.3600 - val_acc: 0.8704\n",
      "Epoch 827/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.1837 - acc: 0.9344 - val_loss: 0.3641 - val_acc: 0.8667\n",
      "Epoch 828/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.1801 - acc: 0.9290 - val_loss: 0.3593 - val_acc: 0.8704\n",
      "Epoch 829/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1771 - acc: 0.9235 - val_loss: 0.3616 - val_acc: 0.8593\n",
      "Epoch 830/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.1845 - acc: 0.9235 - val_loss: 0.3596 - val_acc: 0.8741\n",
      "Epoch 831/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1818 - acc: 0.9344 - val_loss: 0.3643 - val_acc: 0.8741\n",
      "Epoch 832/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1743 - acc: 0.9454 - val_loss: 0.3632 - val_acc: 0.8667\n",
      "Epoch 833/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.1801 - acc: 0.9290 - val_loss: 0.3605 - val_acc: 0.8667\n",
      "Epoch 834/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.1750 - acc: 0.9344 - val_loss: 0.3618 - val_acc: 0.8667\n",
      "Epoch 835/1000\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1717 - acc: 0.968 - 0s 251us/step - loss: 0.1759 - acc: 0.9399 - val_loss: 0.3640 - val_acc: 0.8704\n",
      "Epoch 836/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1759 - acc: 0.9344 - val_loss: 0.3655 - val_acc: 0.8778\n",
      "Epoch 837/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1749 - acc: 0.9399 - val_loss: 0.3547 - val_acc: 0.8778\n",
      "Epoch 838/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.1740 - acc: 0.9454 - val_loss: 0.3564 - val_acc: 0.8778\n",
      "Epoch 839/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1724 - acc: 0.9399 - val_loss: 0.3671 - val_acc: 0.8556\n",
      "Epoch 840/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1731 - acc: 0.9290 - val_loss: 0.3665 - val_acc: 0.8667\n",
      "Epoch 841/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1700 - acc: 0.9399 - val_loss: 0.3624 - val_acc: 0.8667\n",
      "Epoch 842/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1763 - acc: 0.9235 - val_loss: 0.3628 - val_acc: 0.8630\n",
      "Epoch 843/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.1704 - acc: 0.9235 - val_loss: 0.3647 - val_acc: 0.8667\n",
      "Epoch 844/1000\n",
      "183/183 [==============================] - 0s 299us/step - loss: 0.1744 - acc: 0.9290 - val_loss: 0.3647 - val_acc: 0.8630\n",
      "Epoch 845/1000\n",
      "183/183 [==============================] - 0s 286us/step - loss: 0.1700 - acc: 0.9290 - val_loss: 0.3671 - val_acc: 0.8630\n",
      "Epoch 846/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.1699 - acc: 0.9344 - val_loss: 0.3602 - val_acc: 0.8704\n",
      "Epoch 847/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1695 - acc: 0.9399 - val_loss: 0.3605 - val_acc: 0.8704\n",
      "Epoch 848/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.1687 - acc: 0.9399 - val_loss: 0.3632 - val_acc: 0.8630\n",
      "Epoch 849/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.1680 - acc: 0.9454 - val_loss: 0.3640 - val_acc: 0.8778\n",
      "Epoch 850/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.1680 - acc: 0.9454 - val_loss: 0.3596 - val_acc: 0.8778\n",
      "Epoch 851/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1659 - acc: 0.9508 - val_loss: 0.3632 - val_acc: 0.8704\n",
      "Epoch 852/1000\n",
      "183/183 [==============================] - 0s 275us/step - loss: 0.1663 - acc: 0.9399 - val_loss: 0.3616 - val_acc: 0.8704\n",
      "Epoch 853/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1684 - acc: 0.9454 - val_loss: 0.3623 - val_acc: 0.8704\n",
      "Epoch 854/1000\n",
      "183/183 [==============================] - 0s 273us/step - loss: 0.1667 - acc: 0.9399 - val_loss: 0.3656 - val_acc: 0.8667\n",
      "Epoch 855/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1690 - acc: 0.9399 - val_loss: 0.3601 - val_acc: 0.8741\n",
      "Epoch 856/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1664 - acc: 0.9454 - val_loss: 0.3649 - val_acc: 0.8815\n",
      "Epoch 857/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1646 - acc: 0.9399 - val_loss: 0.3688 - val_acc: 0.8593\n",
      "Epoch 858/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1658 - acc: 0.9344 - val_loss: 0.3636 - val_acc: 0.8778\n",
      "Epoch 859/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1658 - acc: 0.9508 - val_loss: 0.3651 - val_acc: 0.8741\n",
      "Epoch 860/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1649 - acc: 0.9508 - val_loss: 0.3648 - val_acc: 0.8704\n",
      "Epoch 861/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1644 - acc: 0.9454 - val_loss: 0.3659 - val_acc: 0.8667\n",
      "Epoch 862/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.1644 - acc: 0.9399 - val_loss: 0.3618 - val_acc: 0.8704\n",
      "Epoch 863/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1640 - acc: 0.9563 - val_loss: 0.3620 - val_acc: 0.8704\n",
      "Epoch 864/1000\n",
      "183/183 [==============================] - 0s 255us/step - loss: 0.1636 - acc: 0.9344 - val_loss: 0.3685 - val_acc: 0.8519\n",
      "Epoch 865/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.1689 - acc: 0.9344 - val_loss: 0.3621 - val_acc: 0.8741\n",
      "Epoch 866/1000\n",
      "183/183 [==============================] - 0s 278us/step - loss: 0.1622 - acc: 0.9399 - val_loss: 0.3686 - val_acc: 0.8630\n",
      "Epoch 867/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1708 - acc: 0.9235 - val_loss: 0.3669 - val_acc: 0.8667\n",
      "Epoch 868/1000\n",
      "183/183 [==============================] - 0s 246us/step - loss: 0.1630 - acc: 0.9454 - val_loss: 0.3639 - val_acc: 0.8815\n",
      "Epoch 869/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1659 - acc: 0.9399 - val_loss: 0.3671 - val_acc: 0.8704\n",
      "Epoch 870/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1656 - acc: 0.9344 - val_loss: 0.3660 - val_acc: 0.8667\n",
      "Epoch 871/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1625 - acc: 0.9563 - val_loss: 0.3654 - val_acc: 0.8889\n",
      "Epoch 872/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.1609 - acc: 0.9454 - val_loss: 0.3660 - val_acc: 0.8741\n",
      "Epoch 873/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1600 - acc: 0.9399 - val_loss: 0.3677 - val_acc: 0.8741\n",
      "Epoch 874/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.1589 - acc: 0.9454 - val_loss: 0.3673 - val_acc: 0.8778\n",
      "Epoch 875/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.1597 - acc: 0.9454 - val_loss: 0.3696 - val_acc: 0.8778\n",
      "Epoch 876/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1599 - acc: 0.9454 - val_loss: 0.3665 - val_acc: 0.8704\n",
      "Epoch 877/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1599 - acc: 0.9399 - val_loss: 0.3674 - val_acc: 0.8667\n",
      "Epoch 878/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.1573 - acc: 0.9454 - val_loss: 0.3716 - val_acc: 0.8741\n",
      "Epoch 879/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1616 - acc: 0.9344 - val_loss: 0.3740 - val_acc: 0.8667\n",
      "Epoch 880/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 282us/step - loss: 0.1620 - acc: 0.9344 - val_loss: 0.3675 - val_acc: 0.8704\n",
      "Epoch 881/1000\n",
      "183/183 [==============================] - 0s 300us/step - loss: 0.1577 - acc: 0.9508 - val_loss: 0.3594 - val_acc: 0.8852\n",
      "Epoch 882/1000\n",
      "183/183 [==============================] - 0s 280us/step - loss: 0.1557 - acc: 0.9617 - val_loss: 0.3651 - val_acc: 0.8741\n",
      "Epoch 883/1000\n",
      "183/183 [==============================] - 0s 315us/step - loss: 0.1568 - acc: 0.9399 - val_loss: 0.3699 - val_acc: 0.8704\n",
      "Epoch 884/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1548 - acc: 0.9399 - val_loss: 0.3718 - val_acc: 0.8815\n",
      "Epoch 885/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.1568 - acc: 0.9617 - val_loss: 0.3663 - val_acc: 0.8741\n",
      "Epoch 886/1000\n",
      "183/183 [==============================] - 0s 301us/step - loss: 0.1529 - acc: 0.9563 - val_loss: 0.3698 - val_acc: 0.8667\n",
      "Epoch 887/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1589 - acc: 0.9399 - val_loss: 0.3704 - val_acc: 0.8815\n",
      "Epoch 888/1000\n",
      "183/183 [==============================] - 0s 280us/step - loss: 0.1512 - acc: 0.9563 - val_loss: 0.3748 - val_acc: 0.8741\n",
      "Epoch 889/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1552 - acc: 0.9454 - val_loss: 0.3673 - val_acc: 0.8815\n",
      "Epoch 890/1000\n",
      "183/183 [==============================] - 0s 227us/step - loss: 0.1543 - acc: 0.9454 - val_loss: 0.3671 - val_acc: 0.8704\n",
      "Epoch 891/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.1583 - acc: 0.9399 - val_loss: 0.3720 - val_acc: 0.8815\n",
      "Epoch 892/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.1529 - acc: 0.9399 - val_loss: 0.3685 - val_acc: 0.8741\n",
      "Epoch 893/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1541 - acc: 0.9563 - val_loss: 0.3715 - val_acc: 0.8815\n",
      "Epoch 894/1000\n",
      "183/183 [==============================] - 0s 261us/step - loss: 0.1543 - acc: 0.9508 - val_loss: 0.3712 - val_acc: 0.8667\n",
      "Epoch 895/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1528 - acc: 0.9563 - val_loss: 0.3729 - val_acc: 0.8778\n",
      "Epoch 896/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.1499 - acc: 0.9563 - val_loss: 0.3711 - val_acc: 0.8704\n",
      "Epoch 897/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1499 - acc: 0.9508 - val_loss: 0.3695 - val_acc: 0.8704\n",
      "Epoch 898/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1536 - acc: 0.9563 - val_loss: 0.3693 - val_acc: 0.8852\n",
      "Epoch 899/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1543 - acc: 0.9399 - val_loss: 0.3758 - val_acc: 0.8667\n",
      "Epoch 900/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1544 - acc: 0.9454 - val_loss: 0.3715 - val_acc: 0.8815\n",
      "Epoch 901/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.1527 - acc: 0.9617 - val_loss: 0.3701 - val_acc: 0.8778\n",
      "Epoch 902/1000\n",
      "183/183 [==============================] - 0s 222us/step - loss: 0.1514 - acc: 0.9563 - val_loss: 0.3716 - val_acc: 0.8778\n",
      "Epoch 903/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1522 - acc: 0.9563 - val_loss: 0.3772 - val_acc: 0.8815\n",
      "Epoch 904/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.1482 - acc: 0.9617 - val_loss: 0.3709 - val_acc: 0.8778\n",
      "Epoch 905/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.1491 - acc: 0.9454 - val_loss: 0.3687 - val_acc: 0.8630\n",
      "Epoch 906/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1496 - acc: 0.9617 - val_loss: 0.3702 - val_acc: 0.8815\n",
      "Epoch 907/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1497 - acc: 0.9563 - val_loss: 0.3742 - val_acc: 0.8889\n",
      "Epoch 908/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1481 - acc: 0.9454 - val_loss: 0.3765 - val_acc: 0.8704\n",
      "Epoch 909/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1504 - acc: 0.9508 - val_loss: 0.3775 - val_acc: 0.8667\n",
      "Epoch 910/1000\n",
      "183/183 [==============================] - 0s 270us/step - loss: 0.1490 - acc: 0.9563 - val_loss: 0.3707 - val_acc: 0.8741\n",
      "Epoch 911/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.1469 - acc: 0.9508 - val_loss: 0.3701 - val_acc: 0.8852\n",
      "Epoch 912/1000\n",
      "183/183 [==============================] - 0s 281us/step - loss: 0.1473 - acc: 0.9617 - val_loss: 0.3723 - val_acc: 0.8852\n",
      "Epoch 913/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1489 - acc: 0.9508 - val_loss: 0.3756 - val_acc: 0.8741\n",
      "Epoch 914/1000\n",
      "183/183 [==============================] - 0s 256us/step - loss: 0.1493 - acc: 0.9454 - val_loss: 0.3702 - val_acc: 0.8852\n",
      "Epoch 915/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1500 - acc: 0.9454 - val_loss: 0.3768 - val_acc: 0.8667\n",
      "Epoch 916/1000\n",
      "183/183 [==============================] - 0s 284us/step - loss: 0.1422 - acc: 0.9617 - val_loss: 0.3733 - val_acc: 0.8815\n",
      "Epoch 917/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1473 - acc: 0.9563 - val_loss: 0.3754 - val_acc: 0.8815\n",
      "Epoch 918/1000\n",
      "183/183 [==============================] - 0s 265us/step - loss: 0.1462 - acc: 0.9508 - val_loss: 0.3741 - val_acc: 0.8852\n",
      "Epoch 919/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1431 - acc: 0.9563 - val_loss: 0.3756 - val_acc: 0.8704\n",
      "Epoch 920/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.1502 - acc: 0.9454 - val_loss: 0.3738 - val_acc: 0.8704\n",
      "Epoch 921/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1408 - acc: 0.9563 - val_loss: 0.3727 - val_acc: 0.8778\n",
      "Epoch 922/1000\n",
      "183/183 [==============================] - 0s 264us/step - loss: 0.1456 - acc: 0.9508 - val_loss: 0.3744 - val_acc: 0.8889\n",
      "Epoch 923/1000\n",
      "183/183 [==============================] - 0s 304us/step - loss: 0.1447 - acc: 0.9508 - val_loss: 0.3772 - val_acc: 0.8667\n",
      "Epoch 924/1000\n",
      "183/183 [==============================] - 0s 293us/step - loss: 0.1477 - acc: 0.9454 - val_loss: 0.3717 - val_acc: 0.8852\n",
      "Epoch 925/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.1443 - acc: 0.9399 - val_loss: 0.3810 - val_acc: 0.8630\n",
      "Epoch 926/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1431 - acc: 0.9399 - val_loss: 0.3796 - val_acc: 0.8778\n",
      "Epoch 927/1000\n",
      "183/183 [==============================] - 0s 302us/step - loss: 0.1378 - acc: 0.9563 - val_loss: 0.3792 - val_acc: 0.8815\n",
      "Epoch 928/1000\n",
      "183/183 [==============================] - 0s 277us/step - loss: 0.1480 - acc: 0.9454 - val_loss: 0.3790 - val_acc: 0.8815\n",
      "Epoch 929/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1377 - acc: 0.9563 - val_loss: 0.3807 - val_acc: 0.8741\n",
      "Epoch 930/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1438 - acc: 0.9399 - val_loss: 0.3767 - val_acc: 0.8704\n",
      "Epoch 931/1000\n",
      "183/183 [==============================] - 0s 253us/step - loss: 0.1355 - acc: 0.9617 - val_loss: 0.3778 - val_acc: 0.8815\n",
      "Epoch 932/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1457 - acc: 0.9617 - val_loss: 0.3825 - val_acc: 0.8815\n",
      "Epoch 933/1000\n",
      "183/183 [==============================] - 0s 258us/step - loss: 0.1391 - acc: 0.9617 - val_loss: 0.3792 - val_acc: 0.8667\n",
      "Epoch 934/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1407 - acc: 0.9563 - val_loss: 0.3744 - val_acc: 0.8741\n",
      "Epoch 935/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.1353 - acc: 0.9672 - val_loss: 0.3770 - val_acc: 0.8778\n",
      "Epoch 936/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.1393 - acc: 0.9672 - val_loss: 0.3815 - val_acc: 0.8815\n",
      "Epoch 937/1000\n",
      "183/183 [==============================] - 0s 290us/step - loss: 0.1350 - acc: 0.9617 - val_loss: 0.3816 - val_acc: 0.8852\n",
      "Epoch 938/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1342 - acc: 0.9508 - val_loss: 0.3821 - val_acc: 0.8741\n",
      "Epoch 939/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 238us/step - loss: 0.1373 - acc: 0.9508 - val_loss: 0.3753 - val_acc: 0.8778\n",
      "Epoch 940/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1389 - acc: 0.9672 - val_loss: 0.3726 - val_acc: 0.8889\n",
      "Epoch 941/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1390 - acc: 0.9617 - val_loss: 0.3820 - val_acc: 0.8778\n",
      "Epoch 942/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.1373 - acc: 0.9617 - val_loss: 0.3875 - val_acc: 0.8741\n",
      "Epoch 943/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.1362 - acc: 0.9617 - val_loss: 0.3779 - val_acc: 0.8852\n",
      "Epoch 944/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1413 - acc: 0.9563 - val_loss: 0.3799 - val_acc: 0.8593\n",
      "Epoch 945/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1365 - acc: 0.9563 - val_loss: 0.3803 - val_acc: 0.8852\n",
      "Epoch 946/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1530 - acc: 0.9563 - val_loss: 0.3920 - val_acc: 0.8741\n",
      "Epoch 947/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1483 - acc: 0.9399 - val_loss: 0.3839 - val_acc: 0.8741\n",
      "Epoch 948/1000\n",
      "183/183 [==============================] - 0s 283us/step - loss: 0.1361 - acc: 0.9563 - val_loss: 0.3691 - val_acc: 0.8815\n",
      "Epoch 949/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1381 - acc: 0.9508 - val_loss: 0.3799 - val_acc: 0.8778\n",
      "Epoch 950/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.1314 - acc: 0.9563 - val_loss: 0.3886 - val_acc: 0.8815\n",
      "Epoch 951/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.1358 - acc: 0.9617 - val_loss: 0.3877 - val_acc: 0.8778\n",
      "Epoch 952/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.1360 - acc: 0.9672 - val_loss: 0.3796 - val_acc: 0.8741\n",
      "Epoch 953/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1303 - acc: 0.9672 - val_loss: 0.3886 - val_acc: 0.8704\n",
      "Epoch 954/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1378 - acc: 0.9508 - val_loss: 0.3840 - val_acc: 0.8926\n",
      "Epoch 955/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1286 - acc: 0.9617 - val_loss: 0.3806 - val_acc: 0.8741\n",
      "Epoch 956/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1347 - acc: 0.9563 - val_loss: 0.3803 - val_acc: 0.8741\n",
      "Epoch 957/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1320 - acc: 0.9617 - val_loss: 0.3836 - val_acc: 0.8889\n",
      "Epoch 958/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1316 - acc: 0.9617 - val_loss: 0.3764 - val_acc: 0.8778\n",
      "Epoch 959/1000\n",
      "183/183 [==============================] - 0s 288us/step - loss: 0.1315 - acc: 0.9508 - val_loss: 0.3856 - val_acc: 0.8778\n",
      "Epoch 960/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.1299 - acc: 0.9617 - val_loss: 0.3840 - val_acc: 0.8778\n",
      "Epoch 961/1000\n",
      "183/183 [==============================] - 0s 236us/step - loss: 0.1287 - acc: 0.9672 - val_loss: 0.3810 - val_acc: 0.8704\n",
      "Epoch 962/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.1287 - acc: 0.9617 - val_loss: 0.3821 - val_acc: 0.8778\n",
      "Epoch 963/1000\n",
      "183/183 [==============================] - 0s 312us/step - loss: 0.1280 - acc: 0.9672 - val_loss: 0.3838 - val_acc: 0.8852\n",
      "Epoch 964/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1253 - acc: 0.9672 - val_loss: 0.3818 - val_acc: 0.8778\n",
      "Epoch 965/1000\n",
      "183/183 [==============================] - 0s 254us/step - loss: 0.1277 - acc: 0.9617 - val_loss: 0.3781 - val_acc: 0.8741\n",
      "Epoch 966/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.1249 - acc: 0.9617 - val_loss: 0.3848 - val_acc: 0.8741\n",
      "Epoch 967/1000\n",
      "183/183 [==============================] - 0s 257us/step - loss: 0.1274 - acc: 0.9563 - val_loss: 0.3886 - val_acc: 0.8704\n",
      "Epoch 968/1000\n",
      "183/183 [==============================] - 0s 230us/step - loss: 0.1265 - acc: 0.9563 - val_loss: 0.3843 - val_acc: 0.8815\n",
      "Epoch 969/1000\n",
      "183/183 [==============================] - 0s 287us/step - loss: 0.1268 - acc: 0.9617 - val_loss: 0.3836 - val_acc: 0.8889\n",
      "Epoch 970/1000\n",
      "183/183 [==============================] - 0s 268us/step - loss: 0.1332 - acc: 0.9563 - val_loss: 0.3896 - val_acc: 0.8741\n",
      "Epoch 971/1000\n",
      "183/183 [==============================] - 0s 241us/step - loss: 0.1265 - acc: 0.9563 - val_loss: 0.3894 - val_acc: 0.8815\n",
      "Epoch 972/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1268 - acc: 0.9672 - val_loss: 0.3896 - val_acc: 0.8778\n",
      "Epoch 973/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1241 - acc: 0.9617 - val_loss: 0.3823 - val_acc: 0.8704\n",
      "Epoch 974/1000\n",
      "183/183 [==============================] - 0s 279us/step - loss: 0.1232 - acc: 0.9672 - val_loss: 0.3820 - val_acc: 0.8926\n",
      "Epoch 975/1000\n",
      "183/183 [==============================] - 0s 252us/step - loss: 0.1223 - acc: 0.9727 - val_loss: 0.3846 - val_acc: 0.8741\n",
      "Epoch 976/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.1222 - acc: 0.9563 - val_loss: 0.3920 - val_acc: 0.8778\n",
      "Epoch 977/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1210 - acc: 0.9672 - val_loss: 0.3868 - val_acc: 0.8778\n",
      "Epoch 978/1000\n",
      "183/183 [==============================] - 0s 283us/step - loss: 0.1216 - acc: 0.9672 - val_loss: 0.3818 - val_acc: 0.8741\n",
      "Epoch 979/1000\n",
      "183/183 [==============================] - 0s 270us/step - loss: 0.1245 - acc: 0.9672 - val_loss: 0.3850 - val_acc: 0.8741\n",
      "Epoch 980/1000\n",
      "183/183 [==============================] - 0s 282us/step - loss: 0.1206 - acc: 0.9672 - val_loss: 0.3896 - val_acc: 0.8852\n",
      "Epoch 981/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1219 - acc: 0.9672 - val_loss: 0.3967 - val_acc: 0.8741\n",
      "Epoch 982/1000\n",
      "183/183 [==============================] - 0s 274us/step - loss: 0.1222 - acc: 0.9617 - val_loss: 0.3908 - val_acc: 0.8704\n",
      "Epoch 983/1000\n",
      "183/183 [==============================] - 0s 296us/step - loss: 0.1208 - acc: 0.9617 - val_loss: 0.3868 - val_acc: 0.8815\n",
      "Epoch 984/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1189 - acc: 0.9672 - val_loss: 0.3868 - val_acc: 0.8778\n",
      "Epoch 985/1000\n",
      "183/183 [==============================] - 0s 280us/step - loss: 0.1206 - acc: 0.9617 - val_loss: 0.3886 - val_acc: 0.8778\n",
      "Epoch 986/1000\n",
      "183/183 [==============================] - 0s 244us/step - loss: 0.1189 - acc: 0.9672 - val_loss: 0.3917 - val_acc: 0.8815\n",
      "Epoch 987/1000\n",
      "183/183 [==============================] - 0s 264us/step - loss: 0.1167 - acc: 0.9672 - val_loss: 0.3864 - val_acc: 0.8741\n",
      "Epoch 988/1000\n",
      "183/183 [==============================] - 0s 249us/step - loss: 0.1183 - acc: 0.9617 - val_loss: 0.3850 - val_acc: 0.8741\n",
      "Epoch 989/1000\n",
      "183/183 [==============================] - 0s 263us/step - loss: 0.1203 - acc: 0.9672 - val_loss: 0.3924 - val_acc: 0.8741\n",
      "Epoch 990/1000\n",
      "183/183 [==============================] - 0s 285us/step - loss: 0.1170 - acc: 0.9672 - val_loss: 0.3925 - val_acc: 0.8741\n",
      "Epoch 991/1000\n",
      "183/183 [==============================] - 0s 269us/step - loss: 0.1164 - acc: 0.9617 - val_loss: 0.3917 - val_acc: 0.8741\n",
      "Epoch 992/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1180 - acc: 0.9672 - val_loss: 0.3926 - val_acc: 0.8741\n",
      "Epoch 993/1000\n",
      "183/183 [==============================] - 0s 247us/step - loss: 0.1146 - acc: 0.9617 - val_loss: 0.3856 - val_acc: 0.8741\n",
      "Epoch 994/1000\n",
      "183/183 [==============================] - 0s 238us/step - loss: 0.1148 - acc: 0.9672 - val_loss: 0.3915 - val_acc: 0.8778\n",
      "Epoch 995/1000\n",
      "183/183 [==============================] - 0s 266us/step - loss: 0.1144 - acc: 0.9727 - val_loss: 0.3895 - val_acc: 0.8815\n",
      "Epoch 996/1000\n",
      "183/183 [==============================] - 0s 271us/step - loss: 0.1182 - acc: 0.9672 - val_loss: 0.3914 - val_acc: 0.8778\n",
      "Epoch 997/1000\n",
      "183/183 [==============================] - 0s 233us/step - loss: 0.1138 - acc: 0.9563 - val_loss: 0.3952 - val_acc: 0.8741\n",
      "Epoch 998/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 274us/step - loss: 0.1151 - acc: 0.9617 - val_loss: 0.3955 - val_acc: 0.8778\n",
      "Epoch 999/1000\n",
      "183/183 [==============================] - 0s 230us/step - loss: 0.1159 - acc: 0.9672 - val_loss: 0.3891 - val_acc: 0.8741\n",
      "Epoch 1000/1000\n",
      "183/183 [==============================] - 0s 260us/step - loss: 0.1134 - acc: 0.9672 - val_loss: 0.3951 - val_acc: 0.8778\n"
     ]
    }
   ],
   "source": [
    "D_in, D_out = data_train_x.shape[1], data_train_x.shape[0]\n",
    "model = Sequential()\n",
    "model.add(Dense(units=D_in, activation='linear', input_shape=(data_x.shape[1],)))\n",
    "model.add(Dense(units=30, activation='tanh'))\n",
    "model.add(Dense(units=30, activation='tanh'))\n",
    "model.add(Dense(units=20, activation='tanh'))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "history = model.fit(x=data_x, y=data_y_oh, epochs=1000, verbose=1, validation_data=(data_test_x, data_test_y_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2931fa69e80>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29315f758d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x29320aa6a20>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x293157227f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(data_test_x)\n",
    "\n",
    "print(accuracy_score(data_test_y, y_pred))\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo o dataset\n",
    "\n",
    "Para iniciar, deve-se analisar os atributos de entrada do dataset, seus tipos e o atributo alvo (label/rótulo). Isso pode ser feito através do Pandas, biblioteca de Python específica para análise e preprocessamento de dados. Isso pode ser feito através da biblioteca Pandas, que permite analisar e preprocessar dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Passo 1 - Leitura do dataset \n",
    "train = None\n",
    "test  = None\n",
    "# Passo 2 - Separar atributos e classes\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_test =  None\n",
    "y_test =  None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificando problemas\n",
    "\n",
    "Mesmo com uma base de dados previamente definida, muitas vezes existem problemas que não foram tratados nela, sendo necessário analisar manualmente. Dentre os problemas, para bases de texto, três são bastante comuns:\n",
    "* Instâncias com informações faltando (NaN) para determinados atributos\n",
    "* Dados discrepantes e outliers\n",
    "* Dados desbalanceados\n",
    "\n",
    "Dessa forma, deve-se itentificar a presença desses problemas no dataset. \n",
    "- Dica 1: utilize funções das bibliotecas do Pandas para os dois primeiros problemas.  \n",
    "- Dica 2: visualize a distribuição de instâncias por classe através de bibliotecas gráficas de Python (e.g. matplob, seaborn e pyplot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solucionando Problemas\n",
    "\n",
    "Os problemas podem ser solucionados utilizando algumas medidas estatísticas como média, mediana e moda para substituir as informações que estão faltando e dados discrepantes, ou excluir as instâncias. Além disso, quanto mais balanceado o dataset, menos propensa estará a rede a cometer erros de generalização. A resolução pode ser feita das duas formas utilizando o Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlação dos atributos\n",
    "\n",
    "Uma vez que o dataset foi analisado e possíveis erros foram corrigidos, é importante também verificar a correlação dos atributos. A correlação é uma métrica estatística que mede a associação entre os atributos. Caso existam atributos altamente correlacionados, pode-se excluir alguns deles, permanecendo apenas um dos atributos correlacionados.\n",
    "\n",
    "Dessa forma, nessa etapa, calcule a correlação entre os atributos. Considere utilizar algumas formas de visualização para melhor interpretação dos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando a Rede Neural\n",
    "\n",
    "Uma vez que a base de dados foi analisada e possíveis problemas foram corrigidos, pode-se implementar a rede neural. Para esse problema, é necessário uma arquitetura multicamada MLP de no máximo 6 camadas escondidas, recomendando-se a implementação das camadas de forma gradual. Da mesma forma, cada camada deverá conter no máximo 30 neurônios. Além disso, recomenda-se a utilização de técnicas de regularização como Dropout, normalização do batch e normalização L2.\n",
    "\n",
    "Utilize o otimizador de sua escolha para o treinamento por batch, aumente a quantidade de epochs e batch também gradativamente, não é necessário uma quantidade muito alta para resolução do problema. Ao final, mostre a curva de treinamento e a matriz de confusão obtida para o problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
